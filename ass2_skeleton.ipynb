{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wv9q_pcGshE"
      },
      "source": [
        "# Assignment 2 2AMM10 2023-2024\n",
        "\n",
        "## Group: [Shash_Kas_Pim]\n",
        "### Member 1: [Shashank Prabhu]\n",
        "### Member 2: [Kasra Gheytuli]\n",
        "### Member 3: [Pim de Wildt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQzvuDWw_Eyw"
      },
      "source": [
        "We need to install some specific libraries. The cell below installs torch_geometric for torch 2.6.0+cu124. In case the current version of torch is different, check [here](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html) to see which versions (of both libraries) you should install. You might also need to install an old version of torch from [here](https://pytorch.org/get-started/previous-versions/)\n",
        "\n",
        "**Note:** Do not install pyg_lib from the optional dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ibC2lMHfD67H"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 2.7.0+cu128\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: C:\\Users\\shash\\OneDrive - TU Eindhoven\\Shashank Prabhu University\\Masters DSAI\\Q4\\2AMM10 Deep Learning\\2AMM10-Assignments\\venv\\Lib\\site-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
            "Required-by: torchaudio, torchvision\n"
          ]
        }
      ],
      "source": [
        "!pip show torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8qrPQFNe_AJu"
      },
      "outputs": [],
      "source": [
        "# !pip install rdkit\n",
        "# !pip install torch_geometric\n",
        "# !pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WVL2eo0g_Iuv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, AllChem\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H8rvaK56_iQ7"
      },
      "outputs": [],
      "source": [
        "dir_add = 'ass2_data/'\n",
        "\n",
        "with open(dir_add+'pos_data.pkl', 'rb') as f:\n",
        "    pos_data = pickle.load(f)\n",
        "\n",
        "with open(dir_add+'type_data.pkl', 'rb') as f:\n",
        "    type_data = pickle.load(f)\n",
        "\n",
        "with open(dir_add+'smiles.pkl', 'rb') as f:\n",
        "    smiles_data = pickle.load(f)\n",
        "\n",
        "data_split = np.load(dir_add+'data_split.npz')\n",
        "\n",
        "train_idxes = data_split['train_idx']\n",
        "test_idxes = data_split['test_idx']\n",
        "\n",
        "formation_energy = np.load(dir_add+'formation_energy.npz')\n",
        "\n",
        "fe = formation_energy['y'] # normalized formation energy\n",
        "mu = formation_energy['mu']\n",
        "std = formation_energy['sigma']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt4tiz7OF74p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DIsGRQcxA_4Q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of data\n",
            "pos_data: 129012, type_data: 129012, smiles: 129012\n",
            "Idxes\n",
            "train: 119012, test: 10000, sum: 129012\n"
          ]
        }
      ],
      "source": [
        "# shapes of lists\n",
        "print(\"Length of data\")\n",
        "print(f\"pos_data: {len(pos_data)}, type_data: {len(type_data)}, smiles: {len(smiles_data)}\")\n",
        "print(\"Idxes\")\n",
        "print(f\"train: {len(train_idxes)}, test: {len(test_idxes)}, sum: {len(train_idxes) + len(test_idxes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bVDJF7I3BFa2"
      },
      "outputs": [],
      "source": [
        "def at_number_to_atom_name(at_number):\n",
        "    if at_number == 6:\n",
        "        return 'C'\n",
        "    elif at_number == 1:\n",
        "        return 'H'\n",
        "    elif at_number == 7:\n",
        "        return 'N'\n",
        "    elif at_number == 8:\n",
        "        return 'O'\n",
        "    elif at_number == 9:\n",
        "        return 'F'\n",
        "    elif at_number == 16:\n",
        "        return 'S'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "def inspect_structure(idx):\n",
        "    smile = smiles_data[idx]\n",
        "    pos = pos_data[idx]\n",
        "    typ = type_data[idx]\n",
        "\n",
        "    header = f\"{'Atom':^5}│{'Number':^6}│{'x':^10}│{'y':^10}│{'z':^10}\"\n",
        "    line   = \"─────┼──────┼──────────┼──────────┼──────────\"\n",
        "    print(header)\n",
        "    print(line)\n",
        "\n",
        "    for atom_num, (x, y, z) in zip(typ, pos):\n",
        "        atom_sym = at_number_to_atom_name(atom_num)\n",
        "        print(f\"{atom_sym:^5}│{atom_num:^6}│{x:>10.3f}│{y:>10.3f}│{z:>10.3f}\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f'SMILE: {smile}')\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f'Formation Energy: {fe[idx]*std + mu:.3f}')\n",
        "    print(f'Formation Energy (normalized): {fe[idx]:.5f}')\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "    if mol:\n",
        "        # RDKit prefers 2‑D coordinates for nice depictions\n",
        "        Chem.AllChem.Compute2DCoords(mol)\n",
        "        img = Draw.MolToImage(mol, size=(300, 300))\n",
        "\n",
        "        # Display with matplotlib (works both in notebooks and scripts)\n",
        "        plt.figure(figsize=(3, 3))\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "K1rs7hhCC4oq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atom │Number│    x     │    y     │    z     \n",
            "─────┼──────┼──────────┼──────────┼──────────\n",
            "  C  │  6   │    -0.013│     1.086│     0.008\n",
            "  H  │  1   │     0.002│    -0.006│     0.002\n",
            "  H  │  1   │     1.012│     1.464│     0.000\n",
            "  H  │  1   │    -0.541│     1.447│    -0.877\n",
            "  H  │  1   │    -0.524│     1.438│     0.906\n",
            "\n",
            "\n",
            "SMILE: C\n",
            "\n",
            "\n",
            "Formation Energy: -17.172\n",
            "Formation Energy (normalized): 5.72327\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# methane\n",
        "# Note how methane has a relatively high formation energy (compared to QM9)\n",
        "# This correlates with lower thermodynamic stability and higher reactivity\n",
        "# For example, methane readily burns in oxygen (CH₄ + 2O₂ → CO₂ + 2H₂O)\n",
        "inspect_structure(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "vo8hYLuQCeBR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Atom │Number│    x     │    y     │    z     \n",
            "─────┼──────┼──────────┼──────────┼──────────\n",
            "  C  │  6   │    -0.241│     1.621│    -0.131\n",
            "  C  │  6   │     0.014│     0.122│    -0.039\n",
            "  C  │  6   │     1.171│    -0.587│    -0.816\n",
            "  C  │  6   │     2.435│     0.094│    -0.260\n",
            "  C  │  6   │     2.081│     0.077│     1.271\n",
            "  O  │  8   │     2.019│     1.352│     1.891\n",
            "  C  │  6   │     0.693│    -0.569│     1.193\n",
            "  C  │  6   │     1.015│    -1.753│     0.215\n",
            "  O  │  8   │     0.024│    -2.723│    -0.005\n",
            "  H  │  1   │     0.642│     2.211│     0.114\n",
            "  H  │  1   │    -0.578│     1.887│    -1.140\n",
            "  H  │  1   │    -1.031│     1.916│     0.568\n",
            "  H  │  1   │    -0.933│    -0.387│    -0.256\n",
            "  H  │  1   │     1.120│    -0.765│    -1.892\n",
            "  H  │  1   │     3.345│    -0.470│    -0.488\n",
            "  H  │  1   │     2.566│     1.118│    -0.619\n",
            "  H  │  1   │     2.772│    -0.570│     1.831\n",
            "  H  │  1   │     2.912│     1.712│     1.918\n",
            "  H  │  1   │     0.171│    -0.730│     2.137\n",
            "  H  │  1   │     1.924│    -2.322│     0.433\n",
            "  H  │  1   │    -0.815│    -2.287│    -0.187\n",
            "\n",
            "\n",
            "SMILE: C[C@H]1[C@H]2C[C@@H](O)[C@@H]1[C@H]2O\n",
            "\n",
            "\n",
            "Formation Energy: -85.637\n",
            "Formation Energy (normalized): -0.91674\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD7CAYAAABDsImYAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAANVBJREFUeJzt3XdYFNfeB/DvLHXpnaVaQAUJiorGhhprbPdqbFGTa0k0JsY30WuMLYk9iSUxllhjLDGWJJbYNWJXNKAoqFho0lk67AJb5vf+MVmuBRCRLbjn8zz7qOzszgH57ilzzhmOiAgMw7zSRPouAMMw2seCzjBGgAWdYYwACzrDGAEWdIYxAizoDGMEWNAZxgiwoDOMEWBBZxgjwILOMEaABZ1hjAALOsMYARZ0hjECLOgMYwRY0BnGCLCgM4wRYEFnGCPAgs4wRoAFnWGMAAs6wxgBFnSGMQIs6AxjBFjQGcYIsKAzjBFgQWcYI8CCzjBGgAWdYYwACzrDGAEWdIYxAizoDGMEWNAZxgiwoDOMEWBBZxgjwILOMEaABZ1hjAALOsMYARZ0hjECLOgMYwRY0BnGCLCgM4wRYEFnGCNgqu8CMIaJiJ74N8dxzz2uqmMY/WM1OlOpgoICTJ06FU2aNMGpU6eqPC4+Ph59+/ZFYGAgSkpKdFhC5kWwoDOVUqvVyM7ORkJCAmQyWZXHKZVKpKSkICEhATzP67CEzItgQWcYI8CCzjBGgAWdYYwACzrDGAF2eY2pFhHht99+Q2xsbKXPS6VSSKVSHZeKeVEs6MxzHT16FOHh4ZU+p1KpUFBQAJGINQ4NGQs6Uy2O4zBhwgS0a9eu0ufT0tKwYsUKZGVl6bhkzItgQWeeq2PHjhg8eHClz929exebNm1iQTdwrL3FMEaABZ1hjAALOsMYARZ0hjECbDCOqZRIJIKjoyM8PT1haWlZ5XGmpqZwc3NDSUkJW6ZqwDh6euExwwBQKBSIjY1Feno6QkNDIZFIKj2uuLgYUVFRkMlk6N27N8zMzHRcUqYmWNCZatV0AwrGsLGmO1OthIQEPHz4EI6OjrC1tYWZmRnMzc1hZmYGa2tr2NrasvDXAyzoTLU0TfOcnBwoFIongt66dWsMGjSIBb0eYE13pkaICEqlEjKZDDKZDHK5HGKxGN7e3izo9QCr0Zlq3blzB3v37oWTkxNcXV3h7u4Od3d3uLi4wNnZmYW8nmA1OlOt3Nxc3LhxA1lZWZBKpcjOzkZ2djZsbGwwc+bMKkfjGcPCgs5Ui+d58DwPU1PTZ77OcRyr0esJ1nRnqpWUlIRJkybB0tISbm5u8PDwgIeHB3r27ImmTZvqu3hMDbEanakWEUGhUCA3NxdSqRSZmZnIzc1F69atERAQoO/iMTXEgs5Ui+d5REdHw9raGvb29nBwcICFhQVrstczrOleh4io4iYGIpGo2tsYqdXqimMMOTQKhQIHDhxATk4OioqKUFRUhHbt2mHmzJnP9NsZw8Vq9DqUl5eH5s2bw9vbG7t374a/v3+lxx05cgSjRo3C0KFDsXTpUjg7O+u4pDVHRCgvL68YlON5HkQEOzs7g/6AYp7EPpLrEBGhuLgYMpms2tsTKZVKFBUVobS09Jm55IYoPDwcRARbW1vY29sjICCAhbyeYUFnnqugoABSqRRyuRwikQg+Pj6wsLDQd7GYF8CCzjzX4MGDoVarQUQgIlhbW+u7SMwLYkFnnis8PBx5eXmwtrZGaGgobG1t9V0k5gWxraSY57K2toZKpUJaWhoKCwv1XRymFliNrgVZWVlYuHAh7O3tK30+MTFRxyV6OZ07d0anTp3AcRy7I0s9xYKuBcXFxTh9+nSV2yrJ5XIdl+jl7Nu3Dzdv3kSbNm3Qv39/NhBXD7Gga4GHhwcWLFgALy+vSp+/cuUKvvrqKx2Xqvbat28PT09P2NjYwMTERN/FqZbmOv/zWh9qtRpA9RObXiUs6FpgbW2Njh07VrnoQyaT6bhEL8fDwwNeXl71IhS///47fvrpJ7z11lsYO3Zsla2PWbNm4cGDB5g0aRL69Omj41LqHutwMc+1d+9ejBo1Crdv39Z3UZ4rKSkJJ0+exP3796udtBQZGYnw8HCkp6frsHT6w2p0plocx2HUqFF4++239V0U5iWwGp2pFhEhJycHSUlJBt8/Z6qmnaArlYBcDigUVR9DJDwvlwP/DIxUeoxaDZSWAiUlQHExIJMJryMSHozW5ebmGk0T91Wlnab76dPAnj3AG28A//lP5cfIZMDOnUBEBDBlCtC69ZPP8zyQnAxcvAhcvgwkJQmBd3YGgoKAsDCgbVvAwUEr30JtiEQi+Pn5wcPDA+bm5lUeZ2NjA39/f0gkknpRS/r5+aFZs2b6LgbzErQT9NhYYOtWwMKi6qCXlwPnzwO//gr8+99PBl2lAs6dA1asAP76C7CyAho3BqytgZgY4PBhYMMG4J13gE8+AXx9tfJtvCgbGxusW7cOFhYWcHd3r/K4Vq1a4aeffoKbmxtsbGx0WMIXU1ZWhvDwcJw/fx4BAQEYOHCgQS+pfVxMTAx+/vnnKucyGFsLxfAG44iEMH/2mfCBMXgw8H//B3h6AqamQq1+7Rrw1VfAjz8K3YT584WaXc+XfszMzNCpU6cqn9csSXV0dERYWJjBXqpSq9WIj4/HsmXLcOTIEUilUtjb2yM6Ohrz58+vF2vRL1y4gMjIyCrLWVRUBCsrKx2XSn8ML+hlZcDy5cDNm8CoUcD69UKNrvkPIwL8/YHmzYH+/YGffgK6dwf+9S+9B/1xmrAolUqoVCoolUooFArI5XIolUp069YNYrFY38V8glqtRmZmJg4ePIglS5agvLwccrkcPj4+6N27N7Zv3460tDQsX74cPj4+Bj0dtnPnzhg0aFCVNfp3332HjIwMHZdKfwwv6GlpQtPcy0vouz8eckD4O8cBwcHApEnAvHnArl1A375CV8FAqFQq7Ny5EyqV6onZWmKxGI6OjujcubO+i1iBiCCXy3H06FFs2bIFERER6NWrF3x9ffHLL79g5MiR+O9//4vGjRtjxYoVmDx5MhYsWICQkBCDDXuLFi0wfvz4Kj9Md+/ezYJep150ZPzKFWF0vVs3wMen6lra1FQ4xsFB6M8rlQYVdHNzc3z00UcwMzN74n5lmqakITR9NV2JmJgYLFu2DKdPn4a9vT2WL1+OLl26YO3atVAqlRg3bhycnJwwefJkuLm5YebMmZgyZQpWrlyJ0NBQg/hemOppN+jp6cCZM5U/V1QEZGU9+/U7d4QPh4YNgSpWfwEQPgBcXABvb+DuXSA7GzCwgS2VSoXY2FgUFxejqKio4k9bW1tMnjxZrwFRq9VIT0/H1q1bsXnzZhARxo8fj4kTJ8LT0xMxMTE4duwYhg4dCh8fHwDC1N7Ro0fD2dkZn3/+OYYNG4bNmzcjLCyMLXQxcNoN+qFDwuNFaNY7W1sLtXZ1zMyEpj0RUFBQqyJqCxEhISEB586dg42NTcV+a97e3tWOyOuiXHl5eTh9+jRWr16Nu3fvonv37vj4448rBghVKhVOnTqF/Px8jBgx4olLhWZmZujXrx/EYjG+/PJLjB8/HvPnz8fw4cPZzjMGTLtBDwsDRo6s/Dm5XLjW/vffT5XonyJVM0+5AtH/jqti0EVfRCIRwsLCEBYWpu+iVFCr1bhw4QI2bdqEkydPokmTJli5ciV69+4NV1fXihaGTCbDzz//jE6dOqF58+bPtDxMTEzQo0cPODk5Yfbs2fjiiy+QlZWFadOmVTt/gNEf7QY9MBCYOLHy5/LygKioZ4Pu6ir8mZ8vjMBX9YtDJFxqKygARCKhGW9g8vPzsWfPHty9exeFhYUoLCxEUVERevfujRkzZuik6a7Zaz4rKwtr1qzBzz//DHNzc0yePBkTJ06Em5vbE/uzExEOHTqE7OxsTJ06FW5ubpW+L8dxaNWqFVatWoXZs2fj22+/RV5eHr744gvY2NjorVtiamoKsVhc5Wi7hoWFBcRicb2YsFQXtBt0jgOq+kGKRJUPtLVrJ3z9/n0h7HZ2Vb9/WhqQmgo0ayYMyhUUCOMC3t7Vv05HLC0tERoaihYtWsDOzg729vawtbXV2XVoIkJWVhaOHTuGH374AZmZmXjzzTfxySefoGXLlpXePKK4uBibN29GkyZN0KdPn2pH1TmOQ5MmTbBmzRosWrQImzdvRn5+PubMmQNfX1+9jMj36dMHLi4uaN68ebVhnzZtGnJyctC2bVsdlk5/DO/yWkiIMNoeHS1MnPHxET4UnqZQAL//LtT6/foJTf7jx4FvvwVatAAGDBC6DnqcSCMWixESEgKZTIaSkhLIZDLk5OTA19cXTk5OWg17aWkpzp07hy1btiA8PBwhISH47LPP8O9//7va2XhnzpzBvXv38OGHH8K3BjMOOY6DRCLBokWL4ODggM2bNyMvLw/ffPMN/P39dV6zBwYGVtrdeFqvXr10VCLDYHhBd3ICxo0DFi4EFi0SQuvt/WRYeR747Tch6A0aCLPnTE2FY7t0EabfHjki1PRvvy2ME1hZVd2K0KJTp05hy5YtEIlEsLa2hrW1NcaMGQMnJyetnI/neaSmpmL58uU4cOAAeJ7H7NmzMWTIEHh7e1fbVC0pKcHhw4ehVCrxn//854VCam9vjxkzZsDR0RHLli3De++9h/Xr1yMwMFCnYZ8xYwbMzc0RFhaGdu3awc7ODiKRqKJ1YbSXAkkbli0T1pZ98EHVx+TkEI0aJRy3f///vs7zRMnJRMOGEVlYEAUEEP34I9HVq0Q3bxKdPk00eTKRnR2RmxvRmjVE5eX/e61KRZSRQbRkCVHbtkSurkR+fkSLFxPduEFUVKSVb7kqKpWKysvLqby8nBQKBSkUClKr1cTzfJ2fJz09nbZs2UINGjQgT09PGjduHCUmJpJKpXru+Xiep4sXL5K/vz9NmTKF1Gr1C5eB53lSKBT0yy+/kL+/P7Vs2ZJOnjxJ5Zr/Hx0oKCig48eP09y5c2nAgAE0YcIEWr16NYWHh1NZWZnOymFoDC/oGvfvE336KZGvL5GJCZGVFZGzM5GZGZFYTNShA9FPPxHJZJW/P88TpaURbd1KNHgwkYcHUcOGRFOmEB05Ipy/jsNWGaVSSdHR0XTixAnav38/7dq1i4rq+MNGJpPRb7/9RgMHDiQHBwcaOHAgHTx4kEpKSmr8HmVlZbRgwQJydXWly5cvv9QHEc/zdPToUQoODqZmzZrRzp07SaVS1fr9aqq0tJTu379Psn9+J0pLSykqKoq2bNlCc+bModzcXK2XwVBp5yaLd+4A168DTZsKg2uVKS8HIiOBxESga1ehL/604mKhnx4XJyxZlcmEUfmmTYUR/SZNqh7s0yASJtPExAjN+V9/FS7FtWwJDBsGvPUWoLkhgRaadQqFAnv37kVGRgYsLS1haWmJESNGwNbW9qWakZr/tri4OKxYsQJHjx6Fo6MjpkyZgn79+sHHx6fG70//bC7RrVs3BAcHY+3atS+9Sk2tVuP8+fOYM2cOUlNT8cUXX2DcuHEwMTHRWvM5MzMTq1evRmJiIpo2bYqOHTuiffv2sLW1hUqlgqmpKWu61ymeJ1Krq68xeb7mx6lUQvO8rIxIoRBeU5sylZYSJSQQLVhA9NprQrM+KIho6VKiBw+E1kEd1/I8z1NZWRnJ5XKSyWQkk8leuumuVqspIyODli1bRo0bNyaJRELTpk2jhIQEUiqVL/zePM/T5s2bydHRkX755ZdaNdurKmdcXBz16dOH3NzcaNGiRZSfn1/n3ZbHz1dcXEyPHj2iAwcO0PTp06lHjx703//+l5KSkrRyzvrCeG+bnJ0NHDggPK5fF+bJDx8O9OkjrI13dKyzGj49PR3h4eGQy+Wwt7fHgAEDajWLjP6Z1Xbu3DmsW7cOkZGR6NatGz788EP06tWr1rVVcXExevbsCWtra2zdurVGo+0vQjOZ5uTJkxg9ejRmzpwJiURSp+cAgIcPH+LRo0fw8PCAt7c3bGxsUFZWhtTUVEgkEqO+lZThjbrripsbMGGCsOnFtWvAiRPAjh3A7t3A668DgwYJy2AdHV/6VJrPUjs7O7i4uNRqkgYRISIiAuvWrcPx48fRqFGjilltEonkpZqkp0+fRmJiIqZOnQpvb+9av09V3NzcsHTpUri4uGDHjh0oLCzEggULKubQ15Xy8nJER0fj0KFDkMvl8PPzQ8eOHdGmTRuDWxKsc3ptTxgKnicqLhZG9WfMIGrQgMjVlUq6daOLmzdTZmYmlZeX17rJyfM8lZeXk1wup7Kyshq/D8/zpFKpKCMjgxYtWkReXl7k7e1Nc+fOpUePHpFSqaxVeR5XUlJCb7/9Nnl7e1NiYuJLv19VeJ6ngoICWrx4MTk5OdHAgQMpNja2Tq9AKBQKKioqooyMDLp9+zbt3buXli5dShkZGXXy/vUZC7qGZsyA54mys4m++46iBwyg1xs3Jl9fX5o9ezZdvny5Vn1Mnufpjz/+oLlz59Lu3buptLS0Rq/Lysqi7du3U5s2bcjZ2ZlGjBhBERERNbpcVtNy/fXXX9SwYUOaNm2a1vrOj5+P53natGkTeXt7U5s2bejixYt1MiagVqvp5MmTtGfPHoqIiKDExESSy+UV5zR2xttHfx6ehzQzE+Hnz+P4iRM4deoUzM3N0bVrV/Tp0wfdu3d/YiFIdYgI9+7dQ35+PiQSCXx9fattvpeVleHSpUvYsmULjh8/jpCQEIwePRpDhw6FXR1O7ZXL5Vi0aBE2btyIv/76q2JarLaVl5dj//79WLBgASwsLLB48WL07t37iTn3L0qtVuP69eu4cuUKcnNzoVQqYWlpiVmzZj133rtR0PMHjcHjeZ5yc3Pp4sWLNH36dPLw8CA3Nzfq0qULrVy5kvLz82vU/NRMJqmqNuZ5vmI0ffr06dSwYUNyd3enZcuWUVJSUp00058+X0JCAr322ms0atSoOr+2/zxlZWV09uxZCgwMpCZNmtDOnTtfunukUqlIqVRSXl4ePXz4kG7cuFFnVxDqOxb0GuJ5npRKJWVkZNC3335Lbdu2JVdXV/Lx8aGvvvqKrl+/Xm2zPiMjg/r160c7d+58JrSafvj27dvJ39+fJBIJjRo1iuLi4uqsmf40tVpNq1evJicnJ/rzzz/10rzleZ7u3r1LHTt2JFdXV1q5ciUVFxfX6r1SUlJo4cKFtGTJEjp+/DjFx8eTQqFgzfZ/sKZ7LRARMjMz8ddff+Hw4cO4ePEiRCIR+vbti4EDB6Jdu3Zwc3N7ohnM8zxUKtUT20kBQjP9xIkT2LJlCy5cuICOHTti3Lhx6NOnj1a3gi4uLkaXLl3g4eGBjRs3amW0vSaICHFxcZg3bx7++usvfPrpp/jggw+qXB5bFZVKhZSUFERHR+PWrVtwdnbG6NGj4VgHV010gYiQm5uLlJQUuLu7w9PTs85PwNQSz/MklUrp1KlTNG3aNHJ3dycPDw968803ae3atZSXl1cxGKRWqykxMZGysrIqvvbgwQP6+OOPycfHh/z9/WnlypWUnJysk0Gxffv2kZOTE61YsUIn01OfV57k5GT64IMPyM7Ojj788ENKS0t7oZ9DYWEhZWZmkkqlIpVKRTKZTO/f19M0/++aR2lpKUVHR9PmzZvpww8/pB49elBwcDBt3LixzrscrEavA0SE8vJySKVSbNu2DXv27EFWVhYcHR3x7rvvYtiwYfD09ERsbCxsbW3h5uaGvXv3YvXq1cjPz8ewYcMwdepUNGjQQCfTNGUyGd5++23Ex8fjyJEjaNSokVbPVxNEBJlMhiVLlmD9+vXo2rUrVq1aBW9v7xr9PKKjo7FkyRLwPI9+/fphxIgRBrG1Fc/zUCgUKC0thVwuR2JiIi5evIjLly/j5s2bKCsrg0gkgomJCby9vdGpUycMGzYMbdu2rdNNMVjQtSA7OxuHDx/Gb7/9hhs3bsDMzAzDhg1Dz549KzZ2iIyMRJcuXTB+/HgMHDhQp5s0HD9+HBMnTsTo0aPx9ddf6+y8NVFSUoJNmzZh+fLlCAgIwNKlSxESElKjX3qe55GYmIj09HS8/vrretvWSi6XIz09HRkZGUhJSUFcXByuX7+O6Oho5Ofnw9XVFV5eXvD09ERQUBBatmyJ4OBgNGrUSGs73rCgawn9s7vL1atXcfr0aezbtw+AsMjFw8MDn3zyCfr27fvSs9pelEwmw5dffomtW7fi0qVLCAgI0Nm5a0omk2H//v34/PPP0aBBAyxcuBDdunWrMgRqtRo3btxASkoKQkNDa9wKqCtKpRKpqamIjo5GTEwMHj58iKSkJKSlpSEjIwNubm4IDg5G8+bNERgYCF9fX/j6+sLLywuWlpY6KSsLupZpmqQPHz7EzJkzcfPmTRw6dAghISEvdd24tmWJiYnBqFGj0KZNG2zcuNFgt2lWKpUIDw/Hxx9/DABYs2YNevToUenPTK1WIzY2FocOHYJCocD8+fPrPDz0z957mjvvSKVSREREICIiAlevXkV2djbKyspQXl4Oa2trtGnTBp07d8brr79eMe/eysoKYrFYLyvojHeuu45wHAdra2u0aNECb731Fs6dOwdTU1O9bEqoVqtx6tQpZGRkYMyYMQa9Y6upqSl69eqFbdu2YdasWXjnnXewbNkyDBky5JnFKRzHISAgAK+99lqdhkihUCAvLw85OTnIysrC/fv3K4KdkpICa2truLq6wsXFBT179kT79u3RqlUrBAYGwtzc/Imy6Ht5LAu6Dmg2YWzQoAEcHR0RFRWFli1b6rwcJSUl2LZtG8LCwnS+xdOL0vzMOnTogB9++AFffPEFPv/8c6Snp+Ojjz6Cw2O3yy4rK8Pu3bvh4OCAfv36wdLSslbnVKvVkEqluHfvHuLi4hAfH48HDx4gPj4eSUlJMDU1RUBAADp06IAxY8bAz88PTZs2RaNGjQz+xpMs6Drk5eUFLy8vXLlyBePHj9fpuYkIJ0+eRGpqKiZNmvTC16n1heM4tGzZEj/88APmzJmD5cuXIzs7G19//XVF/9bc3BwdOnSAWq1+bktJ01PVNMWLiopw8+ZNXL16FVevXsWjR4+Qn5+P/Pz8ipbCG2+8gQ4dOqBx48ZwcXGBo6Mj7Ozs6tVW0ayPrkMFBQV47733cPfuXdy6dUunfXS5XI7+/ftDJpNh9+7daNy4sc7OXReICFKpFN988w22bt2Kfv36YcmSJfD29gbP8ygsLISjo2OlW1ir1WrI5XIUFhYiPz8fCQkJuHLlCs6fP4979+4BAKysrGBrawtvb2907NgRHTt2RMuWLWFvbw8TE5OKnXEMudauDqvRdcje3h7+/v44d+4ckpOT4efnp7Nzh4eHIy4uDu+//75BXDd/URzHwc3NDV988QWcnJywatUqFBcXY+HChfDy8sLZs2cxaNAgiESiiuCnpaUhPj4eCQkJePjwIW7duoW7d++ivLwcPj4+aNy4MUaPHo2mTZsiICAAQUFBcHd3N9g7xL4MFnQd4jgOQUFBMDU1xfXr13UWdJlMhiNHjkCtVmPMmDH1tlYCAEdHR0ybNg0ODg74+uuvMXnyZKxcuRIdOnRAXFwcIiMjcf36ddy7dw/Z2dlIT09HSUkJfHx8EBISgsGDB6Np06aQSCTw9PSEs7OzwV55qEss6DoWEhICMzMzXLt2DUOHDtV66DSX1M6ePYvBgwfXy9r8aWKxGBMmTICzszNmz56NAQMGwMLCAmVlZeA4DiYmJvD09ETHjh3RtWtXtGrVCk5OTjA3N4eFhYVRbhLJgq5jzZo1g4ODA2JjYyGXy7U+TVOhUOD06dPIzs7G2LFjX4lmKcdxsLCwwPDhw3H27Fns378fLVq0QEhICIKCgtC2bVs0aNCgXg2WaRsLuo5ZWFggJCQEt2/fRkpKilZnphERCgoKsGfPHrzxxhto2rTpK1WTpaenIy4uDg0bNsRPP/0EV80NOpln1P+P93qoU6dOkEqlSEhI0Pq5NJfUBg4cWG+WbNYEEeHChQu4ffs23n//fRby52BB14NOnTohLy8PDx8+BF+T+8DXUmlpKdauXYuWLVuiS5cur1Rtnpubiz///BNOTk5466239F0cg8eCrgc+Pj5wcXHBgwcPIJfLtXaeM2fOID4+Hj169ICvr+8rE3T6Z7OKs2fPYvTo0XW6j96rigVdD8zMzNCqVSvcv38fBQUFWjmHXC7Hnj17YGFhgbfffvuVGpjieR6//fYbbGxs0KtXL7b5Yw2woOuBubk5Xn/9ddy5cwf5+fmo68mJRIRr167h8uXLeOutt3Q6MUcX0tPT8euvv6J79+51vpDlVcWCrgempqYIDg5GXl4eUlJS6vz9y8rKcPr0aeTn52PcuHGvVBCICFu3boWZmRn69evHmu01xIKuBxzHQSKRwMPDA1FRUXU+IJeVlYVDhw6hR48eaNKkSZ2+t76lpqbi8OHDaNasGbp166bv4tQbLOh64ubmBj8/P1y5cqVOg87zPE6dOoWkpCSMGDECVlZWdfbe+sbzPI4dO4b4+HiMHTsW9vb2+i5SvcGCrieurq7w8/NDVFQUysrK6ux95XI5NmzYgPbt26Nt27avxEw4jezsbBw7dgwuLi4YPHjwK9Ul0bZX57egnrG0tIS/vz/KysoqlkrWhTNnziAhIQE9e/as+73B9YiIEBsbi8uXL2Ps2LEGscNrfcKCriccx6FZs2aws7PD1atXa/0+RAS1Wg2FQoGioiJs3rwZzs7OGDhwoM73pNOm8vJy7Nu3D3Z2dujbt+8r1VLRhVfnN6Eeat68Oezt7XHp0iV89NFHNW6KavYyy8vLQ25uLu7fv49Lly7hypUrSE5ORosWLWBjYwMiemWat6mpqdi7dy/effdd+Pn5vTLfl66woOuRt7c3PD09ce/ePRQUFFQ5F53neeTm5uLevXuIj4+v2NPswYMHSEpKgoWFBfz8/ODm5galUol79+7hs88+w5dffolmzZq9EqHYtm0bLC0t0bNnT9ZsrwUWdD3SzJD7448/8PDhQ4SGhlZMnikuLsbdu3dx6dIlREZGIikpCTk5OcjLy4NCoUDz5s3RrVs3hIaGokmTJnB2dsahQ4cQFRWFFi1a4LvvvkN6ejo2bNhQ71etZWRk4I8//kCLFi3QsWPHev296AsLuh6p1Wq0bt0aW7ZsQXh4OHJzc3Hx4kVEREQgJiYGarUaYrEYVlZWaNCgAXr06IHOnTujVatWFZsTajZRSE9Pr7ghw5QpU+Dt7Y158+ZhyJAh2LhxI0JDQw16e+eqaKa7SqVSzJ0794ndX5maqz9BVyiAkycBc3OgTRvA2bny49LTgchIwMkJaNsWeHqbICIgLw+4cwdISQFkMsDEBHB0BJo1A/z9hXNoA88DhYXAo0fISU3FkdRUnLlyBfn5+Zg7dy6sra3h4+MDX19fDB48GM2aNUNwcDACAwMhkUiqnK9ORLh37x4iIyPxr3/9C2KxGCNHjoSDgwPmzp2LCRMmYN68efjXv/5V77ZNyszMxLFjx+Dh4YH+/fuz2ryW6k/QZTJg2jQhwKtXVx306Gjg//4PCA0FNmx4MuhEwIULwKpVwK1bQEYGUFoKiESAnR3QuDHw5pvA5MmAmxtQF79UZWVAQgIQFSU8Hj4E0tJQVFKCnSoVEk1NYW9vD19fX3z66ad47bXX4OnpCRcXlxov1lAoFNi7dy9kMhl8fX0hEonAcRwGDBgAe3t7zJgxA5999hlyc3PxwQcf1JuwEBEiIyMRFRWFWbNmsemuL6H+BJ0IyM8XQqlUVn2cQiHU2EVFwms0eB44cQJ4/33hfVq1Aj74AGjSRAj7lSvA7t3A8uXCh8XWrcKHSk3LplQC5eWAXA4kJwNXrwLnzgkfKCUlwjFEgEQCdOgA765dsaN1a5CdHebOnYubN2+ibdu2aN68+Qv/aHJzc3H8+HGYmpo+07Tt3LkzNm7ciJkzZ2Lu3LkoLCzE5MmTYW1tbfCBLy4uxp9//glbW1sMGjRI38Wp1+pP0F9WXBwwZw5QUAB8+ikwfbrQXNcYPhwYNQqYMgU4cgRYuhRYvFho1j+NSKips7OBrCyhC3DvnlBj//03kJsrvLeHBxAYKHQJQkOFD5eGDQFTU5gBcINQa4WFheHAgQPIzMys1R1UNm3ahEePHsHX1xdmZmYVr9fsQx4cHIwNGzZg1qxZWLp0KbKysjB9+nR4eHgYbNiJCElJSThy5AjGjh2r85tRvmqMI+hKJXDokBD29u2BqVMrr63btBGe+/RTYO9e4D//ATQ1rFIJpKUBd+8KtfSDB0LNnZwsjAs4OACvvQYMHSoE29dX6Ap4ewNWVs90Ax7/V2BgIJRKJRISEtC1a9cXWjsulUpx6tQpEBHEYnGlk2Q4joOPjw+WL18OBwcH7NixAzk5OVi6dCkkEkmNz6Vru3btgoWFBXr06FHr2ywxAuMIel6eUNOWlQFvvw1Utb+YiQnQs6cwIHfrFnD+PBAQAMyfD5w+DUilQtO8pASwsQFatxa6Am3bAp6eQj/fzg4Qi4UuRg1wHAdnZ2f4+fnhxo0beOedd2ocdCLC4cOHcefOHQCAtbV1tf16d3d3zJ8/Hy4uLvjxxx9RUFCA77//Ho0aNTK4mWZSqRS7du1Cu3btEBoaymrzl1T/gq5SCU3mR48qfz4n58m+OSAE/cEDoVbt0qX693d2FsJ99aowMs9xQGqq0Md//XXh0a6dUNNbWgrPax615ODggObNm+PatWtQKBQ1rr3y8/Nx/vx5FBUVAQA8PT1hY2NT5fEcx8HJyQlz5syBRCLBwoULMWbMGHzzzTfo0KGDwexCw/M8tm/fjtLSUgwdOpStUqsD9S/oGRlC37mq/3ypVBhce1x5uTA4JxIJg2HV4TihdgaES2FKJbBwoVCD29rWzUj8U+zs7BAQEIBDhw6hsLCwRqPLmhsznDlzBmq1GiKRCD4+Ps+dNcZxHMzMzDBhwgRYW1vj66+/xpQpU7Bo0SL069fPIGrOjIwMHD58GA0bNkSvXr0Mokz1nWG112qC54Wmc0FB5Q+Z7NkanecBtVoIek1qLU3zV/M6TbNcS79wpqamaNy4MUxMTBATE1Oj15SVleHMmTPIzMyseA8vLy+IxeIavd7ExAQjRozAd999B57nMXXqVOzZswcqlarOt7Z6EUSEixcv4vbt23jnnXdeqS2q9an+1ei+vsDatUITujKHDwMTJjz5NTMzYUBMpRI+CGxtqz9HYaHwp1isvckzj+E4Do0bN4abmxsuXLiAfv36VXs8ESE3Nxd79uxBeXk5AGE6rZOT0wutWDM3N0fv3r3h6emJiRMnYvr06cjNzcWYMWP0dvktLy8Phw4dgrW1NUaMGKHz87+q6l+NznFC39jauvJHZTO/bGyEJjsREBtb/fsrFMKkFlNTYcRcR/3WRo0awd3dHZcvX67RjjPHjx9HUlJSxb9NTU0hFotfOJwikQjBwcH45ZdfEBoaigULFmD58uUo1HzY6ZBmhl94eDjGjBnDprvWofoX9NpwcRGuZwPAsWPVH/vggTCTzcZGuO6tIy4uLmjQoAGSk5ORnZ1d7bEqlQo7d+58YmcaMzOzWl+C4jgOfn5+WLFiBfr27YtVq1Zh3rx5Fd0CXVGr1di/fz8sLCzQu3dvto1zHTKOoNvYAG+8IVxW27dPGFFXq588hkjo++/ZA9y/L1wLf+MNnRXRxMQEbdq0gUKhwK1bt6o99siRI7hx48YTXzM3N3+p5Zua7sP333+PcePGYdu2bZg+fToSExN11meXSqXYsWMHunXrhqCgIDYIV4eMI+gcB/TtCwweLEx6+eQT4M8/hcku+fnCJbnbt4EVK4R58A4OwIIFQldAh9q2bQuVSoXo6Ogqj5HL5Th69ChKSkqe+LqtrS3c3Nxe6vwcx8HBwQHffPMNZs2ahdOnT+P999/HzZs3tR52IsLPP/8MjuMwcOBAdkmtjtW/wbjasrUFZs8WJs3s3QtMmgS0aCE061UqICkJuHkTaNQImDFDuN6u4xrltddeg7m5Oe7evQuFQlHpstIbN27g0qVLUD/VInFycqqTWW6ay28fffQRHB0dsWjRInz88cdYvHgxwsLCtDaxJiMjAwcPHkSzZs3QtWtXVpvXsfpTo5uaCjPQWrWqftTcyUkYkW/eXHjN43x8hEUr27cD3boJ89MPHhSWvyqVQsD37AHeeUdng3CPs7W1RWBgIB49eoSsrKxnnlcqlYiKikJycvITX+c4Du7u7nCuakVfLdjY2ODdd9/Fhg0b8OjRI0yZMgUnTpyASqWqs3NoEBFOnDiBhIQEjBw5Ek41XUzE1BzVFzxPpFQSqVTC36uiVld/HM8LD5WKSKEgKi0lKisTXqNWV//eWsbzPC1YsICaNGlCFy9efOa5hIQECg0NJQBPPEQiEY0fP554LZRdrVbTtWvXqG3btiSRSGjHjh0kk8nq9BxZWVk0ZMgQaty4MRUUFNTpezOC+lOjc5xQQ5uYVN+kFomqP04zXdXERLi+bmkpXJIzNRVeq+cmY/v27ZGRkYGUlJRn+sXXr1/H/fv3n3kNx3GwsbHRSnNXJBKhTZs2WLt2LUJCQjBt2jRs2LChzm4OSUS4efMmLl26hPHjx1c7hZepvfoTdCPh7+8PS0tLxMfHQ6FQVHxdrVbj559/RnFx8TOv0QRdWzRhX716NcLCwrB48WKsWLEChYWFLz1IV15ejiNHjsDS0hL9+/c3uMU1rwr2UzUgHMfB1tYWwcHBuHnzJmQyWcVzcrkcZmZmCAgIgKOjIywtLStCoRkt1yaRSAQ/Pz9s2rQJI0aMwJo1a/DRRx9BKpW+VNgzMzOxa9cuDBgwgG3jrEXGM+peT1hZWaFly5Y4duwY5HJ5xcCUnZ0dfv/9d6SkpCAyMhL379/HrVu3cPPmTRQUFKBhw4ZaL5tm9duCBQvg6uqKtWvXYvLkyZg3bx4CAwNrVRtv27YNpqam6NOnD2u2a5N+hwiYp6lUKtq0aRNZWVlRVFRUlQNsarWasrOzKSIigo4cOUJpaWk6LWdBQQGtW7eOXF1dqWfPnhQZGUlqtfqF3iMjI4OCgoLozTffJKlUqqWSMkT1aTDOSIhEIvj6+sLe3r7aWzWJRCI4OTkhJCQEvXr1goeHhw5LCdjb22PcuHH44Ycf8ODBA4wePRpXr16t8eU3IsKBAweQnZ2N4cOH1+mlQeZZLOgGhuM4iMViiMVibN++/ZmJMY+7desW5s6di127dumwhP9jbm6O4cOHY926dbCxscHIkSNx4MAByOXy5742Ozsbf/75J1xdXTFkyBDWN9cyFnQDpFQqoVAocOfOnWpvqXz37l0sX74cBw8e1Msaco7jYGJigj59+mDVqlXw9fXF9OnTsXnzZpQ+vfnHY4gI165dw/Xr1zF27FjYPm/ZMPPSWNANkLm5OSwsLKBSqRAREYGysjIoFAoolUqoVCqo1WrwPK/XDSIeJxKJ0KFDB/z4448ICgrCggULsGLFCiiVykrLWFJSgqNHj8Lc3ByDBw/WQ4mNDxt1N0CabZrlcjn69+8PsViMoKAgNG7cGB4eHpBIJPD09KxY/FJUVIQHDx7A0tISFhYWsLCwgLm5ecUlOF00izmOQ1BQENavX4958+ZhxYoVyM7OxqxZs57YqpmIkJycjIMHD2L06NHw9PRkzXYdYEE3YGZmZhg2bBhycnLAcRxSU1Nx+/ZtFBUVIT8/v2JTyEuXLmHo0KFwdHSEo6MjnJycYGdnB2dnZ9jb28PBwQH29vawsbGBvb19xcPBwaFOb9Gk2Vb622+/haOjI7Zu3Yr8/Hx89dVXT1wj3717d0WTv6ZbXzEvhwXdgInFYnz//fcoLS0Fx3FQKpUoLS1FaWkp5HI5jh07hm+++QZ+fn4YOHAgsrOzkZWVhdjYWGRlZUEqlYLn+YrBPQsLC1hZWVX829rauuIDQSKRQCKRwN3dHW5ubnBxcYFEIoGtre1za9ynn3dxccGcOXNgb2+P1atXQyqVYv369WjQoAHy8vIqtnFu1aoVq811hAXdgPE8j/T09Gf2gdMEVbNxYpMmTfDVV1+BiMDzfEUfnud5FBYWIi0tDRkZGUhPT0d6ejpycnKQnp6O1NRUPHjwoOJYzUOtVle8h5WVFTw8PODl5QUPDw94enrC1dUVnp6e8PLygqOjI8zMzGBiYgIzMzOYmprCzMwMYrEYU6dOhbu7OxYvXozhw4fjhx9+QEREBEpKSjB48GC2Sk2HWNANWElJCUJCQp57nGYNeWUz0xwdHaucNUdEKCkpQW5uLvLz85GTk1PxyM7ORk5ODgoKClBcXIyCggKkpqaipKQExcXFkMlkkMlkMDU1hYuLCxwcHODi4gJXV1c4OzvD1dUVbm5usLe3R/fu3fH7779jwoQJKCkpga+vr8FsLW0sWNANmKWlJWbPnl3l1NJbt25h7969tX5/zdx6W1vbSj8MeJ5HeXl5RbBLSkoqHpqgFxcXQyqVIicnB1KpFAUFBUhKSoJUKoVUKoVKpYKVlRV4nsft27fh4uKCadOmsdpcx1jQDZiFhQWmTp1a6U4zgDCo9TJBfx6RSFTRTagMEYGIKi77KZVKqNXqir8rFArk5+cjPT0dGRkZePToETp37ozOnTuDiFiNrkMs6AbO3Ny8yqC/yB7u2qC5DKi5pPc0IkKjRo3QunXrJ74eGxsLiUQC16rugcfUOTZhhtEazQfB4w9AuGecNrakYqrGgs7oXFhYGExMTBAVFVVxpxlGu1jQGZ3iOA6lpaXYu3cvrl+/XtHPZ7SL9dEZnROLxfjggw/A8zyuX78OS0tLtNLhXXGMEQu6AWrUqBHmzJmD0tLSau9Z3rp1a3z//ffw9/evVyPYHMdBrVZj/fr1SExMxMSJEytq9fr0fdQnHLF2k8Ehoop16CYmJlX+8mtmsnEcp7PFK3WFiJCUlAQHB4eKhS7+/v5syaqWsKAzekNEuHTpEg4dOgRzc3NMmTLlpW8rxVSODcYxepWcnIwePXpgxowZUKlUOH78OBuc0wIWdEZvOI7DkCFD0L59e5w6dQoffvgh8vPz2Ui8FrDBOEavOI7Dpk2bkJWVhVWrVsHFxQV///03fHx84OHhUa/GHQwZ66MzeqVWq5Geng5nZ2dkZGRgx44dKC4uxnvvvYfAwEAW9DrCgs7oXWlpKf744w8cPnwY//73v9G1a1c4OTkhIiICHTt2rHKuP1NzrOnO6J1KpYJEIsGKFStgZ2eHmJgYzJ8/H46OjmjVqhULeh1gNTpjEHieR1xcHPbt24e8vDx0794dvXr1Qnx8PMrKyhAcHAwzMzN9F7PeYjU6YxDKy8tx8OBB+Pv7o0uXLnBycsLu3btx5swZjB49GhzHsTXsL4HV6IxBICLIZDIQEW7fvo3169fD19cXEydOhL29PVJSUuDi4sIm1NQSCzpjMFQqFdatW4d79+5hyJAh6NSpE+7fv49jx47h/v37mDZtGgIDA/VdzHqJBZ0xGGq1GufPn0dQUBDEYjF2796NGzduIDQ0FP369YO7uztrutcSCzpjUFQqFf7++2+sWbMGLVq0wMiRI+Hh4cEG4l4SCzpjUDIzM7Fz50507doVrVq1qner8gwVCzpjUDTz3EUiEYgIiYmJiImJQcOGDdGiRYtKQ69UKnHr1i2kpqaiXbt2T9zrjRGwRS2MQdGsrQeEa+tnzpzBpEmT8Ouvv1b5mrKyMmzZsgWTJk2quPEk8yQWdMagyeVyZGZmorCwsMpjNLeeyszMrPZ+8saMBZ1hjAALOsMYARZ0hjECbK47Uy/I5XKkp6dXOppeXFyM0tJSPZSq/mBBZ+qFs2fPYsKECZU+p1KpcPv2bR2XqH5hQWfqhby8PMTExFT6HBGhoKBAtwWqZ1gfnakXRo8ejaSkJCQnJz/ziImJwaBBg/RdRIPGanSmXqjuJhVsmuzzsRqdYYwACzrDGAEWdIYxAizoDGMEWNAZg+bg4AB/f3+4u7tXeYxIJIJEIoG/vz9sbGx0WLr6g61HZwza47+e1Y2s1/Q4Y8WCzjBGgDXdGcYIsKAzjBFgQWcYI8CCzjBGgAWdYYwACzrDGAEWdIYxAizoDGMEWNAZxgiwoDOMEWBBZxgjwILOMEaABZ1hjAALOsMYARZ0hjECLOgMYwRY0BnGCLCgM4wRYEFnGCPAgs4wRoAFnWGMAAs6wxgBFnSGMQIs6AxjBFjQGcYIsKAzjBFgQWcYI8CCzjBGgAWdYYwACzrDGAEWdIYxAizoDGMEWNAZxgiwoDOMEWBBZxgjwILOMEaABZ1hjAALOsMYARZ0hjECLOgMYwRY0BnGCPw/T/fZ+z1XVJwAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# random structure\n",
        "inspect_structure(np.random.choice(range(len(smiles_data))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76MeHNQ_Gd9t"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv736iffCez4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\shash\\OneDrive - TU Eindhoven\\Shashank Prabhu University\\Masters DSAI\\Q4\\2AMM10 Deep Learning\\2AMM10-Assignments\\venv\\Lib\\site-packages\\torch_geometric\\typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "c:\\Users\\shash\\OneDrive - TU Eindhoven\\Shashank Prabhu University\\Masters DSAI\\Q4\\2AMM10 Deep Learning\\2AMM10-Assignments\\venv\\Lib\\site-packages\\torch_geometric\\typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
            "  warnings.warn(\n",
            "c:\\Users\\shash\\OneDrive - TU Eindhoven\\Shashank Prabhu University\\Masters DSAI\\Q4\\2AMM10 Deep Learning\\2AMM10-Assignments\\venv\\Lib\\site-packages\\torch_geometric\\typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.data import Data, DataLoader as GeometricDataLoader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, AllChem\n",
        "from rdkit import RDLogger\n",
        "import rdkit.DataStructs as DataStructs\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "#GNN\n",
        "\n",
        "class GNNMessagePassing(MessagePassing):\n",
        "    '''\n",
        "    Custom message passing layer implementing:\n",
        "    x_i^(k) = γ^k(x_i^(k-1), ⋃_{j∈N(i)} φ^k(x_i^(k-1), x_j^(k-1), e_{j,i}))\n",
        "    \n",
        "    - φ^k: Edge transfer function (processes bond information)\n",
        "    - γ^k: Node update function (updates atom representations)\n",
        "    - ⋃: Message aggregation (mean aggregation for stability)\n",
        "\n",
        "    REF LEC 3\n",
        "    '''\n",
        "    def __init__(self, node_dim, edge_dim, hidden_dim):\n",
        "        super().__init__(aggr='mean') #using mean aggregation\n",
        "        \n",
        "        # Edge transfer function\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * node_dim + edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Node update function\n",
        "        self.node_mlp = nn.Sequential(\n",
        "            nn.Linear(node_dim + hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, node_dim)\n",
        "        )\n",
        "        \n",
        "        self.layer_norm = nn.LayerNorm(node_dim)\n",
        "        \n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "    \n",
        "    def message(self, x_i, x_j, edge_attr):\n",
        "        edge_input = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
        "        return self.edge_mlp(edge_input)\n",
        "    \n",
        "    def update(self, aggr_out, x):\n",
        "        node_input = torch.cat([x, aggr_out], dim=-1)\n",
        "        h = self.node_mlp(node_input)\n",
        "        return self.layer_norm(h + x)  # Skip connection + normalization\n",
        "\n",
        "\n",
        "class MolecularGNN(nn.Module):\n",
        "    '''\n",
        "    Complete GNN architecture for molecular property prediction.\n",
        "    Respects molecular symmetries through distance-based features.\n",
        "    '''\n",
        "    def __init__(self, num_atom_types=20, hidden_dim=128, num_layers=3):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.atom_embedding = nn.Embedding(num_atom_types, hidden_dim)\n",
        "        self.input_projection = nn.Linear(hidden_dim + 3, hidden_dim)\n",
        "        \n",
        "        self.mp_layers = nn.ModuleList([\n",
        "            GNNMessagePassing(hidden_dim, 1, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.output_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "        \n",
        "    def compute_edge_features(self, pos, edge_index): \n",
        "        #for invariance - bond length remains invariant under rotation and translation invarance as ditance is invariant under translation\n",
        "        '''Compute rotation-invariant distance features'''\n",
        "        row, col = edge_index\n",
        "        distances = torch.norm(pos[row] - pos[col], dim=1, keepdim=True)\n",
        "        return distances\n",
        "    \n",
        "    def forward(self, batch_data):\n",
        "        x = batch_data.x\n",
        "        pos = batch_data.pos\n",
        "        edge_index = batch_data.edge_index\n",
        "        batch = batch_data.batch\n",
        "        \n",
        "        # Embed atoms and combine with positions\n",
        "        atom_features = self.atom_embedding(x)\n",
        "        node_features = torch.cat([atom_features, pos], dim=-1)\n",
        "        node_features = self.input_projection(node_features)\n",
        "        \n",
        "        # Compute edge features\n",
        "        edge_features = self.compute_edge_features(pos, edge_index)\n",
        "        \n",
        "        # Message passing with skip connections\n",
        "        for i, mp_layer in enumerate(self.mp_layers):\n",
        "            if i > 0:\n",
        "                node_features = node_features + mp_layer(node_features, edge_index, edge_features)\n",
        "            else:\n",
        "                node_features = mp_layer(node_features, edge_index, edge_features)\n",
        "        \n",
        "        # Global pooling\n",
        "        graph_features = global_mean_pool(node_features, batch)\n",
        "        \n",
        "        return self.output_mlp(graph_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Data pre proocessing (to add edges for a geometric representation)\n",
        "def create_molecular_graph(positions, atom_types, threshold=2.5): #2.5 Angstroms is a common threshold for covalent bonds\n",
        "    '''\n",
        "    Create PyTorch Geometric graph from molecular data.\n",
        "    Connects atoms within threshold distance.\n",
        "    '''\n",
        "    num_atoms = len(atom_types)\n",
        "    positions = np.array(positions)\n",
        "    \n",
        "    # Create edges based on distance\n",
        "    edges = []\n",
        "    for i in range(num_atoms):\n",
        "        for j in range(i + 1, num_atoms):\n",
        "            dist = np.linalg.norm(positions[i] - positions[j])\n",
        "            if dist < threshold:\n",
        "                edges.append([i, j])\n",
        "                edges.append([j, i])\n",
        "    \n",
        "    # Fallback: fully connected if no edges\n",
        "    if not edges:\n",
        "        edges = [[i, j] for i in range(num_atoms) for j in range(num_atoms) if i != j]\n",
        "    \n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    \n",
        "    return Data(\n",
        "        x=torch.tensor(atom_types, dtype=torch.long),\n",
        "        pos=torch.tensor(positions, dtype=torch.float),\n",
        "        edge_index=edge_index\n",
        "    )\n",
        "\n",
        "#Dataset class and loaders\n",
        "class GeometricDataset(Dataset):\n",
        "    '''PyTorch dataset for geometric molecular data'''\n",
        "    def __init__(self, pos_data, type_data, targets, indices, threshold=2.5):\n",
        "        self.graphs = []\n",
        "        print(f\"Creating {len(indices)} molecular graphs with threshold={threshold}...\")\n",
        "        for idx in tqdm(indices):\n",
        "            graph = create_molecular_graph(pos_data[idx], type_data[idx], threshold)\n",
        "            graph.y = torch.tensor([targets[idx]], dtype=torch.float)\n",
        "            self.graphs.append(graph)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAINING AND EVALUATION FUNCTIONS\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, device, is_geometric=True):\n",
        "    '''Train model for one epoch'''\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in tqdm(loader, desc=\"Training batches\", leave=False):\n",
        "        if is_geometric:\n",
        "            batch = batch.to(device)\n",
        "            outputs = model(batch)\n",
        "            targets = batch.y.view(-1, 1)\n",
        "        else:\n",
        "            inputs, masks, targets = batch\n",
        "            inputs = inputs.to(device)\n",
        "            masks = masks.to(device)\n",
        "            targets = targets.to(device).view(-1, 1)\n",
        "            outputs = model(inputs, masks)\n",
        "        \n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device, is_geometric=True):\n",
        "    '''Evaluate model on dataset and return comprehensive metrics'''\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    targets_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if is_geometric:\n",
        "                batch = batch.to(device)\n",
        "                outputs = model(batch)\n",
        "                targets = batch.y\n",
        "            else:\n",
        "                inputs, masks, targets = batch\n",
        "                inputs = inputs.to(device)\n",
        "                masks = masks.to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs = model(inputs, masks).squeeze()\n",
        "            \n",
        "            loss = criterion(outputs.view(-1), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            targets_list.extend(targets.cpu().numpy())\n",
        "    \n",
        "    predictions = np.array(predictions)\n",
        "    targets_list = np.array(targets_list)\n",
        "    \n",
        "    # Calculate all metrics in normalized space\n",
        "    mae_norm = mean_absolute_error(targets_list, predictions)\n",
        "    mse_norm = mean_squared_error(targets_list, predictions)\n",
        "    r2 = r2_score(targets_list, predictions)\n",
        "    \n",
        "    # Unnormalize for reporting (formula: original = normalized * std + mu)\n",
        "    # For MAE and MSE: only multiply by std (since errors are scale-dependent but offset-independent)\n",
        "    mae_unnorm = mae_norm * std\n",
        "    mse_unnorm = mse_norm * (std ** 2)  # MSE scales with variance\n",
        "    \n",
        "    # Double-check unnormalization with a few examples\n",
        "    targets_unnorm = targets_list * std + mu\n",
        "    predictions_unnorm = predictions * std + mu\n",
        "    mae_unnorm_check = mean_absolute_error(targets_unnorm, predictions_unnorm)\n",
        "    mse_unnorm_check = mean_squared_error(targets_unnorm, predictions_unnorm)\n",
        "\n",
        "    return {\n",
        "        'loss': total_loss / len(loader),\n",
        "        'mae_norm': mae_norm,\n",
        "        'mse_norm': mse_norm,\n",
        "        'mae_unnorm': mae_unnorm,\n",
        "        'mse_unnorm': mse_unnorm,\n",
        "        'mae_unnorm_check': mae_unnorm_check,  # Should match mae_unnorm\n",
        "        'mse_unnorm_check': mse_unnorm_check,  # Should match mse_unnorm\n",
        "        'r2': r2,\n",
        "        'predictions': predictions,\n",
        "        'targets': targets_list\n",
        "        }\n",
        "\n",
        "#Helper\n",
        "def generate_model_filename(config):\n",
        "    \"\"\"Generate model filename based on hyperparameters\"\"\"\n",
        "    return f\"gnn_h{config['hidden_dim']}_l{config['num_layers']}_t{config['threshold']:.1f}_lr{config['lr']:.0e}_bs{config['batch_size']}_e{config['num_epochs']}.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMILES PREPROCESSING AND TOKENIZATION\n",
        "\n",
        "def build_char_vocab(smiles_list):\n",
        "    \"\"\"Build character-level vocabulary from SMILES strings\"\"\"\n",
        "    chars = sorted({c for smi in smiles_list for c in smi})\n",
        "    char2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    char2idx.update({c: i+2 for i, c in enumerate(chars)})\n",
        "    idx2char = {idx: char for char, idx in char2idx.items()}\n",
        "    return char2idx, idx2char\n",
        "\n",
        "def tokenize_smiles(smi, char2idx, max_len):\n",
        "    \"\"\"Convert SMILES string to padded sequence of character indices\"\"\"\n",
        "    indices = [char2idx.get(c, char2idx['<UNK>']) for c in smi]\n",
        "    length = min(len(indices), max_len)\n",
        "    \n",
        "    # Pad or truncate to max_len\n",
        "    if len(indices) < max_len:\n",
        "        indices = indices + [char2idx['<PAD>']] * (max_len - len(indices))\n",
        "    else:\n",
        "        indices = indices[:max_len]\n",
        "    \n",
        "    return indices, length\n",
        "\n",
        "def smiles_to_fp(smi, radius=2, nBits=2048):\n",
        "    \"\"\"Convert SMILES to Morgan fingerprint (for future use)\"\"\"\n",
        "    arr = np.zeros((nBits,), dtype=np.uint8)\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    if mol:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits)\n",
        "        rdkit.DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "    return arr\n",
        "\n",
        "# Build vocabulary from training data\n",
        "train_smiles = [smiles_data[i] for i in train_idxes]\n",
        "char2idx, idx2char = build_char_vocab(train_smiles)\n",
        "vocab_size = len(char2idx)\n",
        "max_len = max(len(smi) for smi in smiles_data)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Max sequence length: {max_len}\")\n",
        "print(f\"Sample characters: {list(char2idx.keys())[:20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM MODEL FOR SMILES\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class ImprovedLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM-based model for SMILES sequence processing.\n",
        "    Handles variable-length sequences with proper padding and packing.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim=64, hidden_dim=128, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Character embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            emb_dim, hidden_dim, num_layers,\n",
        "            batch_first=True, \n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        \n",
        "        # Regularization and output\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "    def forward(self, seq, lengths=None):\n",
        "        \"\"\"\n",
        "        Forward pass through LSTM\n",
        "        Args:\n",
        "            seq: (batch_size, seq_len) - tokenized SMILES\n",
        "            lengths: (batch_size,) - actual lengths before padding\n",
        "        \"\"\"\n",
        "        # Embed characters\n",
        "        x = self.embedding(seq)  # (batch_size, seq_len, emb_dim)\n",
        "        \n",
        "        # Pack sequences for efficient LSTM processing\n",
        "        if lengths is not None:\n",
        "            x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        lstm_out, (h_n, _) = self.lstm(x)\n",
        "        \n",
        "        if lengths is not None:\n",
        "            lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        \n",
        "        # Use final hidden state from last layer\n",
        "        last_hidden = h_n[-1]  # (batch_size, hidden_dim)\n",
        "        \n",
        "        # Apply normalization and dropout\n",
        "        out = self.layer_norm(last_hidden)\n",
        "        out = self.dropout(out)\n",
        "        \n",
        "        return self.fc(out).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMILES DATASET CLASS\n",
        "class SMILESDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for SMILES molecular data\"\"\"\n",
        "    def __init__(self, smiles_data, targets, indices, char2idx, max_len):\n",
        "        self.sequences = []\n",
        "        self.lengths = []\n",
        "        self.targets = []\n",
        "        \n",
        "        print(f\"Processing {len(indices)} SMILES sequences...\")\n",
        "        for idx in tqdm(indices, desc=\"Tokenizing SMILES\"):\n",
        "            smi = smiles_data[idx]\n",
        "            seq, length = tokenize_smiles(smi, char2idx, max_len)\n",
        "            \n",
        "            self.sequences.append(seq)\n",
        "            self.lengths.append(length)\n",
        "            self.targets.append(targets[idx])\n",
        "        \n",
        "        # Convert to tensors\n",
        "        self.sequences = torch.tensor(self.sequences, dtype=torch.long)\n",
        "        self.lengths = torch.tensor(self.lengths, dtype=torch.long) \n",
        "        self.targets = torch.tensor(self.targets, dtype=torch.float)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.lengths[idx], self.targets[idx]\n",
        "\n",
        "def smiles_collate_fn(batch):\n",
        "    \"\"\"Custom collate function for SMILES data - sorts by length for packing\"\"\"\n",
        "    sequences, lengths, targets = zip(*batch)\n",
        "    \n",
        "    # Sort by length (descending) for efficient packing\n",
        "    sorted_batch = sorted(zip(sequences, lengths, targets), \n",
        "                         key=lambda x: x[1], reverse=True)\n",
        "    sequences, lengths, targets = zip(*sorted_batch)\n",
        "    \n",
        "    return (torch.stack(sequences), \n",
        "            torch.tensor(lengths), \n",
        "            torch.stack(targets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COMMON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Plotting\n",
        "def plot_results(results, config):\n",
        "    \"\"\"Plot training curves and predictions with enhanced metrics\"\"\"\n",
        "    \n",
        "    val_metrics = results['val_metrics']\n",
        "    test_results = results['test_results']\n",
        "    \n",
        "    # Extract metrics for plotting\n",
        "    val_losses = [m['loss'] for m in val_metrics]\n",
        "    val_maes = [m['mae_unnorm'] for m in val_metrics]\n",
        "    val_mses = [m['mse_unnorm'] for m in val_metrics]\n",
        "    val_r2s = [m['r2'] for m in val_metrics]\n",
        "    \n",
        "    # Plot training curves\n",
        "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss curves\n",
        "    ax1.plot(results['train_losses'], label='Train Loss', color='blue')\n",
        "    ax1.plot(val_losses, label='Val Loss', color='orange')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('MSE Loss (normalized)')\n",
        "    ax1.set_title('Training Curves')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    # MAE progression (unnormalized)\n",
        "    ax2.plot(val_maes, label='Validation MAE', color='green')\n",
        "    ax2.axhline(y=10, color='red', linestyle='--', alpha=0.7, label='Grading Threshold (MAE < 10)')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('MAE (unnormalized)')\n",
        "    ax2.set_title('MAE Progression')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "    \n",
        "    # MSE progression (unnormalized)\n",
        "    ax3.plot(val_mses, label='Validation MSE', color='purple')\n",
        "    ax3.axhline(y=120, color='red', linestyle='--', alpha=0.7, label='Grading Threshold (MSE < 120)')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_ylabel('MSE (unnormalized)')\n",
        "    ax3.set_title('MSE Progression')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True)\n",
        "    \n",
        "    # R² progression\n",
        "    ax4.plot(val_r2s, label='Validation R²', color='brown')\n",
        "    ax4.set_xlabel('Epoch')\n",
        "    ax4.set_ylabel('R² Score')\n",
        "    ax4.set_title('R² Score Progression')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Prediction scatter plot\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Unnormalized predictions\n",
        "    targets_unnorm = test_results['targets'] * std + mu\n",
        "    predictions_unnorm = test_results['predictions'] * std + mu\n",
        "    \n",
        "    ax1.scatter(targets_unnorm, predictions_unnorm, alpha=0.6, s=20)\n",
        "    ax1.plot([targets_unnorm.min(), targets_unnorm.max()], \n",
        "             [targets_unnorm.min(), targets_unnorm.max()], 'r--', lw=2)\n",
        "    ax1.set_xlabel('True Formation Energy')\n",
        "    ax1.set_ylabel('Predicted Formation Energy')\n",
        "    ax1.set_title(f'Test Predictions (R² = {test_results[\"r2\"]:.3f})')\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    # Configuration and results summary\n",
        "    config_text = f\"\"\"Final Configuration:\n",
        "    Hidden Dim: {config['hidden_dim']}\n",
        "    Layers: {config['num_layers']}\n",
        "    Dropout: {config['dropout']}\n",
        "    Threshold: {config['threshold']} Å\n",
        "    Learning Rate: {config['lr']:.0e}\n",
        "    Batch Size: {config['batch_size']}\n",
        "\n",
        "    Final Test Results:\n",
        "    MAE (unnorm): {test_results['mae_unnorm']:.4f}\n",
        "    MSE (unnorm): {test_results['mse_unnorm']:.4f}\n",
        "    R²: {test_results['r2']:.4f}\n",
        "\n",
        "    Training Info:\n",
        "    Epochs trained: {results['epochs_trained']}/{config['num_epochs']}\n",
        "    Model: {results['model_filename']}\"\"\"\n",
        "    \n",
        "    ax2.text(0.05, 0.95, config_text, transform=ax2.transAxes, \n",
        "             verticalalignment='top', fontfamily='monospace', fontsize=9,\n",
        "             bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
        "    ax2.set_xlim(0, 1)\n",
        "    ax2.set_ylim(0, 1)\n",
        "    ax2.axis('off')\n",
        "    ax2.set_title('Experiment Summary')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAIN\n",
        "def run_training(config):\n",
        "    \"\"\"Main training function that uses the configuration\"\"\"\n",
        "    \n",
        "    # Configuration\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Set random seeds\n",
        "    np.random.seed(config['seed'])\n",
        "    torch.manual_seed(config['seed'])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config['seed'])\n",
        "    \n",
        "    # Split training data for validation\n",
        "    train_idx_split, val_idx_split = train_test_split(\n",
        "        train_idxes, test_size=config['val_split'], random_state=config['seed']\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nData split:\")\n",
        "    print(f\"Training: {len(train_idx_split)} molecules\")\n",
        "    print(f\"Validation: {len(val_idx_split)} molecules\")\n",
        "    print(f\"Test: {len(test_idxes)} molecules\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"PART A: TRAINING GNN MODEL\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    # Create datasets and loaders with configurable threshold\n",
        "    train_geo_dataset = GeometricDataset(pos_data, type_data, fe, train_idx_split, config['threshold'])\n",
        "    val_geo_dataset = GeometricDataset(pos_data, type_data, fe, val_idx_split, config['threshold'])\n",
        "    test_geo_dataset = GeometricDataset(pos_data, type_data, fe, test_idxes, config['threshold'])\n",
        "    \n",
        "    train_geo_loader = GeometricDataLoader(train_geo_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_geo_loader = GeometricDataLoader(val_geo_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "    test_geo_loader = GeometricDataLoader(test_geo_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "    \n",
        "    print(f\"Data loading time: {time.time() - start:.2f} seconds\")\n",
        "    \n",
        "    # Initialize model and setup with configurable parameters\n",
        "    gnn_model = MolecularGNN(\n",
        "        num_atom_types=20, \n",
        "        hidden_dim=config['hidden_dim'], \n",
        "        num_layers=config['num_layers']\n",
        "    ).to(device)\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in gnn_model.parameters()):,}\")\n",
        "    \n",
        "    optimizer = torch.optim.Adam(gnn_model.parameters(), lr=config['lr'])\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, \n",
        "        patience=config['scheduler_patience'], \n",
        "        factor=config['scheduler_factor']\n",
        "    )\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop (no early stopping - runs for full num_epochs)\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_metrics = []\n",
        "    \n",
        "    start = time.time()\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        # Train\n",
        "        train_loss = train_epoch(gnn_model, train_geo_loader, optimizer, criterion, device, True)\n",
        "        \n",
        "        # Validate\n",
        "        val_results = evaluate(gnn_model, val_geo_loader, criterion, device, True)\n",
        "        \n",
        "        # Store metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_metrics.append(val_results)\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_results['loss'])\n",
        "        \n",
        "        # Save best model based on validation loss\n",
        "        if val_results['loss'] < best_val_loss:\n",
        "            best_val_loss = val_results['loss']\n",
        "            model_filename = generate_model_filename(config)\n",
        "            torch.save(gnn_model.state_dict(), model_filename)\n",
        "            print(f\"Saved new best model: {model_filename}\")\n",
        "        \n",
        "        # Print progress every few epochs or at the end\n",
        "        if (epoch + 1) % max(1, config['num_epochs'] // 10) == 0 or epoch == config['num_epochs'] - 1:\n",
        "            print(f\"Epoch {epoch+1}/{config['num_epochs']}:\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "            print(f\"  Val Loss: {val_results['loss']:.4f}\")\n",
        "            print(f\"  Val MAE (norm): {val_results['mae_norm']:.4f}, (unnorm): {val_results['mae_unnorm']:.4f}\")\n",
        "            print(f\"  Val MSE (norm): {val_results['mse_norm']:.4f}, (unnorm): {val_results['mse_unnorm']:.4f}\")\n",
        "            print(f\"  Val R²: {val_results['r2']:.4f}\")\n",
        "            print(f\"  Time elapsed: {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    # Load best model and evaluate on test set\n",
        "    model_filename = generate_model_filename(config)\n",
        "    gnn_model.load_state_dict(torch.load(model_filename))\n",
        "    test_results = evaluate(gnn_model, test_geo_loader, criterion, device, True)\n",
        "    \n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"FINAL TEST RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"NORMALIZED METRICS:\")\n",
        "    print(f\"  Test MAE: {test_results['mae_norm']:.4f}\")\n",
        "    print(f\"  Test MSE: {test_results['mse_norm']:.4f}\")\n",
        "    print(f\"  Test R²:  {test_results['r2']:.4f}\")\n",
        "    print(f\"\\nUNNORMALIZED METRICS:\")\n",
        "    print(f\"  Test MAE: {test_results['mae_unnorm']:.4f}\")\n",
        "    print(f\"  Test MSE: {test_results['mse_unnorm']:.4f}\")\n",
        "    print(f\"\\nModel saved as: {model_filename}\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_metrics': val_metrics,\n",
        "        'test_results': test_results,\n",
        "        'model_filename': model_filename,\n",
        "        'epochs_trained': len(train_losses)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Data split:\n",
            "Training: 107110 molecules\n",
            "Validation: 11902 molecules\n",
            "Test: 10000 molecules\n",
            "\n",
            "================================================================================\n",
            "PART A: TRAINING GNN MODEL\n",
            "================================================================================\n",
            "Creating 107110 molecular graphs with threshold=2.5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 107110/107110 [01:13<00:00, 1464.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating 11902 molecular graphs with threshold=2.5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11902/11902 [00:08<00:00, 1453.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating 10000 molecular graphs with threshold=2.5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:06<00:00, 1525.35it/s]\n",
            "c:\\Users\\shash\\OneDrive - TU Eindhoven\\Shashank Prabhu University\\Masters DSAI\\Q4\\2AMM10 Deep Learning\\2AMM10-Assignments\\venv\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loading time: 87.91 seconds\n",
            "Model parameters: 82,817\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved new best model: gnn_h64_l3_t2.5_lr1e-03_bs32_e10.pt\n",
            "Epoch 1/10:\n",
            "  Train Loss: 0.1508\n",
            "  Val Loss: 0.0676\n",
            "  Val MAE (norm): 0.1646, (unnorm): 1.6971\n",
            "  Val MSE (norm): 0.0676, (unnorm): 7.1887\n",
            "  Val R²: 0.9321\n",
            "  Time elapsed: 68.87 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved new best model: gnn_h64_l3_t2.5_lr1e-03_bs32_e10.pt\n",
            "Epoch 2/10:\n",
            "  Train Loss: 0.0313\n",
            "  Val Loss: 0.0258\n",
            "  Val MAE (norm): 0.1004, (unnorm): 1.0352\n",
            "  Val MSE (norm): 0.0258, (unnorm): 2.7419\n",
            "  Val R²: 0.9741\n",
            "  Time elapsed: 135.69 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved new best model: gnn_h64_l3_t2.5_lr1e-03_bs32_e10.pt\n",
            "Epoch 3/10:\n",
            "  Train Loss: 0.0214\n",
            "  Val Loss: 0.0207\n",
            "  Val MAE (norm): 0.0855, (unnorm): 0.8813\n",
            "  Val MSE (norm): 0.0207, (unnorm): 2.2041\n",
            "  Val R²: 0.9792\n",
            "  Time elapsed: 200.88 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved new best model: gnn_h64_l3_t2.5_lr1e-03_bs32_e10.pt\n",
            "Epoch 4/10:\n",
            "  Train Loss: 0.0205\n",
            "  Val Loss: 0.0148\n",
            "  Val MAE (norm): 0.0739, (unnorm): 0.7619\n",
            "  Val MSE (norm): 0.0148, (unnorm): 1.5703\n",
            "  Val R²: 0.9852\n",
            "  Time elapsed: 272.13 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved new best model: gnn_h64_l3_t2.5_lr1e-03_bs32_e10.pt\n",
            "Epoch 5/10:\n",
            "  Train Loss: 0.0172\n",
            "  Val Loss: 0.0132\n",
            "  Val MAE (norm): 0.0760, (unnorm): 0.7833\n",
            "  Val MSE (norm): 0.0132, (unnorm): 1.3982\n",
            "  Val R²: 0.9868\n",
            "  Time elapsed: 332.59 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      1\u001b[39m CONFIG = {\n\u001b[32m      2\u001b[39m         \u001b[38;5;66;03m# MODEL ARCHITECTURE\u001b[39;00m\n\u001b[32m      3\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mhidden_dim\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m64\u001b[39m,        \u001b[38;5;66;03m# Hidden dimension (64, 128, 256, 512)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mval_split\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0.1\u001b[39m,         \u001b[38;5;66;03m# Validation split (0.1 = 10%)\u001b[39;00m\n\u001b[32m     18\u001b[39m     }\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Run training with the configuration\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m results = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel saved as: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33mmodel_filename\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(config)\u001b[39m\n\u001b[32m     63\u001b[39m start = time.time()\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[33m'\u001b[39m\u001b[33mnum_epochs\u001b[39m\u001b[33m'\u001b[39m]):\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_geo_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     69\u001b[39m     val_results = evaluate(gnn_model, val_geo_loader, criterion, device, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device, is_geometric)\u001b[39m\n\u001b[32m     20\u001b[39m loss = criterion(outputs, targets)\n\u001b[32m     22\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=\u001b[32m1.0\u001b[39m)\n\u001b[32m     25\u001b[39m optimizer.step()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shash\\OneDrive - TU Eindhoven\\Shashank Prabhu University\\Masters DSAI\\Q4\\2AMM10 Deep Learning\\2AMM10-Assignments\\venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shash\\OneDrive - TU Eindhoven\\Shashank Prabhu University\\Masters DSAI\\Q4\\2AMM10 Deep Learning\\2AMM10-Assignments\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shash\\OneDrive - TU Eindhoven\\Shashank Prabhu University\\Masters DSAI\\Q4\\2AMM10 Deep Learning\\2AMM10-Assignments\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "CONFIG_GNN = {\n",
        "        # MODEL ARCHITECTURE\n",
        "        'hidden_dim': 64,        # Hidden dimension (64, 128, 256, 512)\n",
        "        'num_layers': 3,          # Number of GNN layers (2, 3, 4, 5, 6)\n",
        "        'dropout': 0,             # Dropout rate (0.0, 0.1, 0.2, 0.3)\n",
        "        'threshold': 2.5,         # Distance threshold in Angstroms (2.0, 2.5, 3.0, 3.5, 4.0)\n",
        "        \n",
        "        # TRAINING PARAMETERS\n",
        "        'lr': 1e-3,               # Learning rate (1e-4, 5e-4, 1e-3, 2e-3, 5e-3)\n",
        "        'batch_size': 32,         # Batch size (16, 32, 64, 128)\n",
        "        'num_epochs': 10,         # Number of training epochs\n",
        "        'scheduler_patience': 10, # LR scheduler patience (reduces LR when val loss plateaus)\n",
        "        'scheduler_factor': 0.5,  # LR reduction factor\n",
        "        \n",
        "        # OTHER\n",
        "        'seed': 69,               # Random seed for reproducibility\n",
        "        'val_split': 0.1,         # Validation split (0.1 = 10%)\n",
        "    }\n",
        "    \n",
        "    \n",
        "CONFIG_SMILES = {\n",
        "    # MODEL ARCHITECTURE\n",
        "    'emb_dim': 64,            # Embedding dimension (32, 64, 128)\n",
        "    'hidden_dim': 128,        # LSTM hidden dimension (64, 128, 256)\n",
        "    'num_layers': 2,          # Number of LSTM layers (1, 2, 3)\n",
        "    'dropout': 0.1,           # Dropout rate (0.0, 0.1, 0.2, 0.3)\n",
        "    \n",
        "    # TRAINING PARAMETERS\n",
        "    'lr': 1e-3,               # Learning rate (1e-4, 5e-4, 1e-3, 2e-3)\n",
        "    'weight_decay': 1e-5,     # L2 regularization\n",
        "    'batch_size': 64,         # Batch size (32, 64, 128)\n",
        "    'num_epochs': 25,         # Number of training epochs\n",
        "    'scheduler_patience': 5,  # LR scheduler patience\n",
        "    'scheduler_factor': 0.5,  # LR reduction factor\n",
        "    \n",
        "    # OTHER\n",
        "    'seed': 69,               # Random seed for reproducibility\n",
        "    'val_split': 0.1,         # Validation split (0.1 = 10%)\n",
        "}\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training with the configuration\n",
        "results_GNN = run_training(CONFIG_GNN)\n",
        "print(f\"Model saved as: {results_GNN['model_filename']}\")\n",
        "\n",
        "# Plot results\n",
        "plot_results(results_GNN, CONFIG_GNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training with the configuration\n",
        "results_SMILES = run_training(CONFIG_SMILES)\n",
        "print(f\"Model saved as: {results_SMILES['model_filename']}\")\n",
        "\n",
        "# Plot results\n",
        "plot_results(results_SMILES, CONFIG_SMILES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zvKnpLGf9v"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC1KICrZGgkY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mIuOY4BGxqU"
      },
      "source": [
        "## Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_NgxsO3GxEE"
      },
      "outputs": [],
      "source": [
        "def is_valid_smiles(smiles):\n",
        "    if smiles is None:\n",
        "        return False\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        return mol is not None\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def canonicalize(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol:\n",
        "            return Chem.MolToSmiles(mol, canonical=True)\n",
        "        return 'None'\n",
        "    except:\n",
        "        return 'None'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN_jGgOwG4kK"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('COO', 'COO')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "canonicalize(\"COO\"), canonicalize(\"O(C)O\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bhjYhYrHCuQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, True, False)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "is_valid_smiles(\"COO\"), is_valid_smiles(\"O(C)O\"), is_valid_smiles(\"C##\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_YgzDpMH-Vl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
