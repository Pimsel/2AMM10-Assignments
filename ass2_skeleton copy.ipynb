{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wv9q_pcGshE"
      },
      "source": [
        "# Assignment 2 2AMM10 2023-2024\n",
        "\n",
        "## Group: [Shash_Kas_Pim]\n",
        "### Member 1: [Shashank Prabhu]\n",
        "### Member 2: [Kasra Gheytuli]\n",
        "### Member 3: [Pim de Wildt]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQzvuDWw_Eyw"
      },
      "source": [
        "We need to install some specific libraries. The cell below installs torch_geometric for torch 2.6.0+cu124. In case the current version of torch is different, check [here](https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html) to see which versions (of both libraries) you should install. You might also need to install an old version of torch from [here](https://pytorch.org/get-started/previous-versions/)\n",
        "\n",
        "**Note:** Do not install pyg_lib from the optional dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibC2lMHfD67H"
      },
      "outputs": [],
      "source": [
        "!pip show torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qrPQFNe_AJu"
      },
      "outputs": [],
      "source": [
        "# !pip install rdkit\n",
        "# !pip install torch_geometric\n",
        "# !pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVL2eo0g_Iuv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, AllChem\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8rvaK56_iQ7"
      },
      "outputs": [],
      "source": [
        "dir_add = 'ass2_data/'\n",
        "\n",
        "with open(dir_add+'pos_data.pkl', 'rb') as f:\n",
        "    pos_data = pickle.load(f)\n",
        "\n",
        "with open(dir_add+'type_data.pkl', 'rb') as f:\n",
        "    type_data = pickle.load(f)\n",
        "\n",
        "with open(dir_add+'smiles.pkl', 'rb') as f:\n",
        "    smiles_data = pickle.load(f)\n",
        "\n",
        "data_split = np.load(dir_add+'data_split.npz')\n",
        "\n",
        "train_idxes = data_split['train_idx']\n",
        "test_idxes = data_split['test_idx']\n",
        "\n",
        "formation_energy = np.load(dir_add+'formation_energy.npz')\n",
        "\n",
        "fe = formation_energy['y'] # normalized formation energy\n",
        "mu = formation_energy['mu']\n",
        "std = formation_energy['sigma']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bt4tiz7OF74p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIsGRQcxA_4Q"
      },
      "outputs": [],
      "source": [
        "# shapes of lists\n",
        "print(\"Length of data\")\n",
        "print(f\"pos_data: {len(pos_data)}, type_data: {len(type_data)}, smiles: {len(smiles_data)}\")\n",
        "print(\"Idxes\")\n",
        "print(f\"train: {len(train_idxes)}, test: {len(test_idxes)}, sum: {len(train_idxes) + len(test_idxes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVDJF7I3BFa2"
      },
      "outputs": [],
      "source": [
        "def at_number_to_atom_name(at_number):\n",
        "    if at_number == 6:\n",
        "        return 'C'\n",
        "    elif at_number == 1:\n",
        "        return 'H'\n",
        "    elif at_number == 7:\n",
        "        return 'N'\n",
        "    elif at_number == 8:\n",
        "        return 'O'\n",
        "    elif at_number == 9:\n",
        "        return 'F'\n",
        "    elif at_number == 16:\n",
        "        return 'S'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "def inspect_structure(idx):\n",
        "    smile = smiles_data[idx]\n",
        "    pos = pos_data[idx]\n",
        "    typ = type_data[idx]\n",
        "\n",
        "    header = f\"{'Atom':^5}│{'Number':^6}│{'x':^10}│{'y':^10}│{'z':^10}\"\n",
        "    line   = \"─────┼──────┼──────────┼──────────┼──────────\"\n",
        "    print(header)\n",
        "    print(line)\n",
        "\n",
        "    for atom_num, (x, y, z) in zip(typ, pos):\n",
        "        atom_sym = at_number_to_atom_name(atom_num)\n",
        "        print(f\"{atom_sym:^5}│{atom_num:^6}│{x:>10.3f}│{y:>10.3f}│{z:>10.3f}\")\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f'SMILE: {smile}')\n",
        "    print(\"\")\n",
        "    print(\"\")\n",
        "    print(f'Formation Energy: {fe[idx]*std + mu:.3f}')\n",
        "    print(f'Formation Energy (normalized): {fe[idx]:.5f}')\n",
        "    mol = Chem.MolFromSmiles(smile)\n",
        "    if mol:\n",
        "        # RDKit prefers 2‑D coordinates for nice depictions\n",
        "        Chem.AllChem.Compute2DCoords(mol)\n",
        "        img = Draw.MolToImage(mol, size=(300, 300))\n",
        "\n",
        "        # Display with matplotlib (works both in notebooks and scripts)\n",
        "        plt.figure(figsize=(3, 3))\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1rs7hhCC4oq"
      },
      "outputs": [],
      "source": [
        "# methane\n",
        "# Note how methane has a relatively high formation energy (compared to QM9)\n",
        "# This correlates with lower thermodynamic stability and higher reactivity\n",
        "# For example, methane readily burns in oxygen (CH₄ + 2O₂ → CO₂ + 2H₂O)\n",
        "inspect_structure(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vo8hYLuQCeBR"
      },
      "outputs": [],
      "source": [
        "# random structure\n",
        "inspect_structure(np.random.choice(range(len(smiles_data))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76MeHNQ_Gd9t"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv736iffCez4"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_geometric.data import Data, DataLoader as GeometricDataLoader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw, AllChem\n",
        "from rdkit import RDLogger\n",
        "import rdkit.DataStructs as DataStructs\n",
        "RDLogger.DisableLog('rdApp.*')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#GNN\n",
        "\n",
        "class GNNMessagePassing(MessagePassing):\n",
        "    \"\"\"\n",
        "    Message passing layer that updates node features by:\n",
        "    h_i = UPDATE(old_h_i, mean(messages from neighbors))\n",
        "\n",
        "    where:\n",
        "    - message = EDGE_FUNC(h_i, h_j, edge_features)  \n",
        "    - h_i = atom i's features\n",
        "    - EDGE_FUNC = MLP processing atom pair + bond distance (implemented in 'message' function)\n",
        "    \n",
        "    (Ref lec 3)\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim, edge_dim, hidden_dim):\n",
        "        super().__init__(aggr='mean') # Using mean aggregation for stability\n",
        "        \n",
        "        # Edge transfer function\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * node_dim + edge_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Node update function\n",
        "        self.node_mlp = nn.Sequential(\n",
        "            nn.Linear(node_dim + hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, node_dim)\n",
        "        )\n",
        "        \n",
        "        self.layer_norm = nn.LayerNorm(node_dim)\n",
        "        \n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "    \n",
        "    def message(self, x_i, x_j, edge_attr):\n",
        "        edge_input = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
        "        return self.edge_mlp(edge_input)\n",
        "    \n",
        "    def update(self, aggr_out, x):\n",
        "        node_input = torch.cat([x, aggr_out], dim=-1)\n",
        "        h = self.node_mlp(node_input)\n",
        "        return self.layer_norm(h + x)  # Skip connection + norm\n",
        "\n",
        "\n",
        "class MolecularGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete GNN architecture for molecular property prediction.\n",
        "    it respects molecular symmetries through distance based features\n",
        "    \"\"\"\n",
        "    def __init__(self, num_atom_types=20, hidden_dim=128, num_layers=3):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.atom_embedding = nn.Embedding(num_atom_types, hidden_dim)\n",
        "        self.input_projection = nn.Linear(hidden_dim + 3, hidden_dim)\n",
        "        \n",
        "        self.mp_layers = nn.ModuleList([\n",
        "            GNNMessagePassing(hidden_dim, 1, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.output_mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "        \n",
        "    def compute_edge_features(self, pos, edge_index): \n",
        "        # For invariance - bond length remains invariant under rotation and translation invarance as ditance is invariant under translation\n",
        "        \"\"\"Compute rotation-invariant distance features\"\"\"\n",
        "        row, col = edge_index\n",
        "        distances = torch.norm(pos[row] - pos[col], dim=1, keepdim=True)\n",
        "        return distances\n",
        "    \n",
        "    def forward(self, batch_data):\n",
        "        x = batch_data.x\n",
        "        pos = batch_data.pos\n",
        "        edge_index = batch_data.edge_index\n",
        "        batch = batch_data.batch\n",
        "        \n",
        "        # Embed atoms and combine with positions\n",
        "        atom_features = self.atom_embedding(x)\n",
        "        node_features = torch.cat([atom_features, pos], dim=-1)\n",
        "        node_features = self.input_projection(node_features)\n",
        "        \n",
        "        # Compute edge features\n",
        "        edge_features = self.compute_edge_features(pos, edge_index)\n",
        "        \n",
        "        # Message passing with skip connections\n",
        "        for i, mp_layer in enumerate(self.mp_layers):\n",
        "            if i > 0:\n",
        "                node_features = node_features + mp_layer(node_features, edge_index, edge_features)\n",
        "            else:\n",
        "                node_features = mp_layer(node_features, edge_index, edge_features)\n",
        "        \n",
        "        # Global pooling\n",
        "        graph_features = global_mean_pool(node_features, batch)\n",
        "        \n",
        "        return self.output_mlp(graph_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data pre proocessing (to add edges for a geometric representation)\n",
        "\n",
        "def create_molecular_graph(positions, atom_types, threshold=2.5): #2.5 Angstroms is a common threshold for covalent bonds\n",
        "    \"\"\"\n",
        "    Create PyTorch Geometric graph from molecular data - connects atoms within threshold distance (as they are 'floating' in space).\n",
        "    \"\"\"\n",
        "    num_atoms = len(atom_types)\n",
        "    positions = np.array(positions)\n",
        "    \n",
        "    # Create edges based on distance\n",
        "    edges = []\n",
        "    for i in range(num_atoms):\n",
        "        for j in range(i + 1, num_atoms):\n",
        "            dist = np.linalg.norm(positions[i] - positions[j])\n",
        "            if dist < threshold:\n",
        "                edges.append([i, j])\n",
        "                edges.append([j, i])\n",
        "    \n",
        "    # Fallback: fully connected if no edges (not ideal for realism but ensures connectivity)\n",
        "    if not edges:\n",
        "        edges = [[i, j] for i in range(num_atoms) for j in range(num_atoms) if i != j]\n",
        "    \n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    \n",
        "    return Data(\n",
        "        x=torch.tensor(atom_types, dtype=torch.long),\n",
        "        pos=torch.tensor(positions, dtype=torch.float),\n",
        "        edge_index=edge_index\n",
        "    )\n",
        "\n",
        "\n",
        "#Dataset class and loaders\n",
        "\n",
        "class GeometricDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for geometric molecular data\"\"\"\n",
        "    def __init__(self, pos_data, type_data, targets, indices, threshold=2.5):\n",
        "        self.graphs = []\n",
        "        print(f\"Creating {len(indices)} molecular graphs with threshold={threshold}...\")\n",
        "        for idx in tqdm(indices):\n",
        "            graph = create_molecular_graph(pos_data[idx], type_data[idx], threshold)\n",
        "            graph.y = torch.tensor([targets[idx]], dtype=torch.float)\n",
        "            self.graphs.append(graph)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.graphs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.graphs[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SMILES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMILES PREPROCESSING AND TOKENIZATION\n",
        "\n",
        "def build_char_vocab(smiles_list):\n",
        "    \"\"\"Build character-level vocabulary from SMILES strings\"\"\"\n",
        "    chars = sorted({c for smi in smiles_list for c in smi})\n",
        "    char2idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    char2idx.update({c: i+2 for i, c in enumerate(chars)})\n",
        "    idx2char = {idx: char for char, idx in char2idx.items()}\n",
        "    return char2idx, idx2char\n",
        "\n",
        "def tokenize_smiles(smi, char2idx, max_len):\n",
        "    \"\"\"Convert SMILES string to padded sequence of character indices\"\"\"\n",
        "    indices = [char2idx.get(c, char2idx['<UNK>']) for c in smi]\n",
        "    length = min(len(indices), max_len)\n",
        "    \n",
        "    # Pad or truncate to max_len\n",
        "    if len(indices) < max_len:\n",
        "        indices = indices + [char2idx['<PAD>']] * (max_len - len(indices))\n",
        "    else:\n",
        "        indices = indices[:max_len]\n",
        "    \n",
        "    return indices, length\n",
        "\n",
        "def smiles_to_fp(smi, radius=2, nBits=2048):\n",
        "    \"\"\"Convert SMILES to Morgan fingerprint (for future use)\"\"\"\n",
        "    arr = np.zeros((nBits,), dtype=np.uint8)\n",
        "    mol = Chem.MolFromSmiles(smi)\n",
        "    if mol:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits)\n",
        "        rdkit.DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "    return arr\n",
        "\n",
        "# Build vocabulary from training data\n",
        "train_smiles = [smiles_data[i] for i in train_idxes]\n",
        "char2idx, idx2char = build_char_vocab(train_smiles)\n",
        "vocab_size = len(char2idx)\n",
        "max_len = max(len(smi) for smi in smiles_data)\n",
        "\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "print(f\"Max sequence length: {max_len}\")\n",
        "print(f\"Sample characters: {list(char2idx.keys())[:20]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM MODEL FOR SMILES\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class ImprovedLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    LSTM-based model for SMILES sequence processing.\n",
        "    Handles variable-length sequences with proper padding and packing.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, emb_dim=64, hidden_dim=128, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Character embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
        "        \n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(\n",
        "            emb_dim, hidden_dim, num_layers,\n",
        "            batch_first=True, \n",
        "            dropout=dropout if num_layers > 1 else 0.0,\n",
        "            bidirectional=False\n",
        "        )\n",
        "        \n",
        "        # Regularization and output\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "    def forward(self, seq, lengths=None):\n",
        "        \"\"\"\n",
        "        Forward pass through LSTM\n",
        "        Args:\n",
        "            seq: (batch_size, seq_len) - tokenized SMILES\n",
        "            lengths: (batch_size,) - actual lengths before padding\n",
        "        \"\"\"\n",
        "        # Embed characters\n",
        "        x = self.embedding(seq)  # (batch_size, seq_len, emb_dim)\n",
        "        \n",
        "        # Pack sequences for efficient LSTM processing\n",
        "        if lengths is not None:\n",
        "            x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=True)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        lstm_out, (h_n, _) = self.lstm(x)\n",
        "        \n",
        "        if lengths is not None:\n",
        "            lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
        "        \n",
        "        # Use final hidden state from last layer\n",
        "        last_hidden = h_n[-1]  # (batch_size, hidden_dim)\n",
        "        \n",
        "        # Apply normalization and dropout\n",
        "        out = self.layer_norm(last_hidden)\n",
        "        out = self.dropout(out)\n",
        "        \n",
        "        return self.fc(out).squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMILES DATASET CLASS\n",
        "class SMILESDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for SMILES molecular data\"\"\"\n",
        "    def __init__(self, smiles_data, targets, indices, char2idx, max_len):\n",
        "        self.sequences = []\n",
        "        self.lengths = []\n",
        "        self.targets = []\n",
        "        \n",
        "        print(f\"Processing {len(indices)} SMILES sequences...\")\n",
        "        for idx in tqdm(indices, desc=\"Tokenizing SMILES\"):\n",
        "            smi = smiles_data[idx]\n",
        "            seq, length = tokenize_smiles(smi, char2idx, max_len)\n",
        "            \n",
        "            self.sequences.append(seq)\n",
        "            self.lengths.append(length)\n",
        "            self.targets.append(targets[idx])\n",
        "        \n",
        "        # Convert to tensors\n",
        "        self.sequences = torch.tensor(self.sequences, dtype=torch.long)\n",
        "        self.lengths = torch.tensor(self.lengths, dtype=torch.long) \n",
        "        self.targets = torch.tensor(self.targets, dtype=torch.float)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.lengths[idx], self.targets[idx]\n",
        "\n",
        "def smiles_collate_fn(batch):\n",
        "    \"\"\"Custom collate function for SMILES data - sorts by length for packing\"\"\"\n",
        "    sequences, lengths, targets = zip(*batch)\n",
        "    \n",
        "    # Sort by length (descending) for efficient packing\n",
        "    sorted_batch = sorted(zip(sequences, lengths, targets), \n",
        "                         key=lambda x: x[1], reverse=True)\n",
        "    sequences, lengths, targets = zip(*sorted_batch)\n",
        "    \n",
        "    return (torch.stack(sequences), \n",
        "            torch.tensor(lengths), \n",
        "            torch.stack(targets))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### COMMON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAINING AND EVALUATION FUNCTIONS\n",
        "\n",
        "def train_epoch(model, loader, optimizer, criterion, device, model_type='gnn'):\n",
        "    \"\"\"Train model for one epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    desc = f\"Training {model_type.upper()} batches\"\n",
        "    for batch in tqdm(loader, desc=desc, leave=False):\n",
        "        if model_type == 'gnn':\n",
        "            batch = batch.to(device)\n",
        "            outputs = model(batch)\n",
        "            targets = batch.y.view(-1, 1)\n",
        "        else:  # SMILES\n",
        "            sequences, lengths, targets = batch\n",
        "            sequences = sequences.to(device)\n",
        "            lengths = lengths.to(device)\n",
        "            targets = targets.to(device).view(-1, 1)\n",
        "            outputs = model(sequences, lengths=lengths).view(-1, 1)\n",
        "        \n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(loader)\n",
        "\n",
        "\n",
        "\n",
        "## MODIFIED TO WORK WITH Q2 AS WELL ##\n",
        "\n",
        "def evaluate(model, loader, criterion, device, model_type='gnn', use_custom_metrics=False):\n",
        "    \"\"\"Evaluate model on dataset and return metrics\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    targets_list = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if model_type == 'gnn':\n",
        "                batch = batch.to(device)\n",
        "                outputs = model(batch)\n",
        "                targets = batch.y\n",
        "            else: #SMILES\n",
        "                sequences, lengths, targets = batch\n",
        "                sequences = sequences.to(device)\n",
        "                lengths = lengths.to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs = model(sequences, lengths=lengths)\n",
        "            \n",
        "            loss = criterion(outputs.view(-1), targets.view(-1))\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            predictions.extend(outputs.cpu().numpy())\n",
        "            targets_list.extend(targets.cpu().numpy())\n",
        "    \n",
        "    predictions = np.array(predictions)\n",
        "    targets_list = np.array(targets_list)\n",
        "\n",
        "    results = {'loss': total_loss / len(loader)}\n",
        "\n",
        "    if use_custom_metrics:\n",
        "        # Use custom metrics for Task 2\n",
        "        for metric_name, metric_func in EFFICIENCY_METRIC_CONFIG.items():\n",
        "            try:\n",
        "                value, unit = metric_func(targets_list, predictions, std, mu)\n",
        "                results[metric_name] = {\n",
        "                    'value': value,\n",
        "                    'unit': unit,\n",
        "                    'name': metric_func.__name__.replace('compute_', '').replace('_metric', '')\n",
        "                }\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not compute {metric_name}: {e}\")\n",
        "                results[metric_name] = {\n",
        "                    'value': np.nan,\n",
        "                    'unit': 'Error',\n",
        "                    'name': 'Error'\n",
        "                }\n",
        "    \n",
        "    else: #standard as task 1\n",
        "        # Calculate all metrics in (normalized))\n",
        "        mae_norm = mean_absolute_error(targets_list, predictions)\n",
        "        mse_norm = mean_squared_error(targets_list, predictions)\n",
        "        r2 = r2_score(targets_list, predictions)\n",
        "    \n",
        "        # Unnormalize\n",
        "        # For MAE and MSE: only multiply by std or std^2 respectively (mu cancels out in difference)\n",
        "        mae_unnorm = mae_norm * std\n",
        "        mse_unnorm = mse_norm * (std ** 2)\n",
        "\n",
        "        results.update({\n",
        "            'mae_norm': mae_norm,\n",
        "            'mse_norm': mse_norm,\n",
        "            'mae_unnorm': mae_unnorm,\n",
        "            'mse_unnorm': mse_unnorm,\n",
        "            'r2': r2,\n",
        "        })\n",
        "\n",
        "    results.update({\n",
        "        'predictions': predictions,\n",
        "        'targets': targets_list\n",
        "    })\n",
        "    \n",
        "\n",
        "    return results\n",
        "\n",
        "#Helper\n",
        "def generate_model_filename(config, model_type='gnn'):\n",
        "    \"\"\"Generate model filename based on hyperparameters and model type\"\"\"\n",
        "    if model_type == 'gnn':\n",
        "        return f\"gnn_h{config['hidden_dim']}_l{config['num_layers']}_t{config['threshold']:.1f}_lr{config['lr']:.0e}_bs{config['batch_size']}_e{config['num_epochs']}.pt\"\n",
        "    else:  # SMILES\n",
        "        return f\"lstm_emb{config['emb_dim']}_h{config['hidden_dim']}_l{config['num_layers']}_lr{config['lr']:.0e}_bs{config['batch_size']}_e{config['num_epochs']}.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_results(results, config, model_type='gnn'):\n",
        "    \"\"\"Plot training/validation loss and true vs predicted scatter plot.\"\"\"\n",
        "    \n",
        "    val_metrics = results['val_metrics']\n",
        "    val_losses = [m['loss'] for m in val_metrics]\n",
        "    test_results = results['test_results']\n",
        "    \n",
        "    \n",
        "    # Pllot for loss curves\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
        "    \n",
        "    # Loss curves\n",
        "    ax.plot(results['train_losses'], label='Train Loss', color='blue', linewidth=2)\n",
        "    ax.plot(val_losses, label='Val Loss', color='orange', linewidth=2)\n",
        "    ax.set_xlabel('Epoch', fontsize=12)\n",
        "    ax.set_ylabel('MSE Loss (normalized)', fontsize=12)\n",
        "    ax.set_title(f'{model_type.upper()} Model - Training and Validation Loss', fontsize=14)\n",
        "    ax.legend(fontsize=11)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    final_train_loss = results['train_losses'][-1]\n",
        "    final_val_loss = val_losses[-1]\n",
        "    \n",
        "    ax.text(0.02, 0.98, f'Final Train Loss: {final_train_loss:.4f}\\nFinal Val Loss: {final_val_loss:.4f}', \n",
        "            transform=ax.transAxes, verticalalignment='top', fontsize=10,\n",
        "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Prediction scatter plot\n",
        "    fig, (ax1) = plt.subplots(1, 1, figsize=(10, 6))\n",
        "\n",
        "    # Unnormalized predictions\n",
        "    targets_unnorm = test_results['targets'] * std + mu\n",
        "    predictions_unnorm = test_results['predictions'] * std + mu\n",
        "    \n",
        "    ax1.scatter(targets_unnorm, predictions_unnorm, alpha=0.6, s=20)\n",
        "    ax1.plot([targets_unnorm.min(), targets_unnorm.max()], \n",
        "             [targets_unnorm.min(), targets_unnorm.max()], 'r--', lw=2)\n",
        "    ax1.set_xlabel('True Formation Energy (eV)')\n",
        "    ax1.set_ylabel('Predicted Formation Energy (eV)')\n",
        "    ax1.set_title(f'{model_type.upper()} Test Predictions (R² = {test_results[\"r2\"]:.3f})')\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_training(config, model_type='gnn', subset_indices=None, use_efficiency_metrics=False):\n",
        "    \"\"\"\n",
        "    Unified training function that works for both Task 1 and Task 2\n",
        "    \n",
        "    Args:\n",
        "        config: Training configuration\n",
        "        model_type: 'gnn' or 'smiles'\n",
        "        subset_indices: Optional subset of training indices for Task 2\n",
        "        use_efficiency_metrics: Whether to use custom metrics (Task 2) or standard metrics (Task 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    np.random.seed(config['seed'])\n",
        "    torch.manual_seed(config['seed'])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(config['seed'])\n",
        "    \n",
        "    # Use subset indices if provided (Task 2), otherwise use full training set (Task 1)\n",
        "    training_indices = subset_indices if subset_indices is not None else train_idxes\n",
        "\n",
        "    # Split training data for validation\n",
        "    val_split = config.get('val_split', 0.1)\n",
        "\n",
        "    train_idx_split, val_idx_split = train_test_split(\n",
        "        train_idxes, test_size=val_split, random_state=config['seed']\n",
        "    )\n",
        "    \n",
        "    task_name = f\"Task 2 (subset size: {len(training_indices)})\" if subset_indices is not None else \"Task 1\"\n",
        "    print(f\"\\n{model_type.upper()} Data split for {task_name}:\")\n",
        "    print(f\"Training: {len(train_idx_split)} molecules\")\n",
        "    print(f\"Validation: {len(val_idx_split)} molecules\")\n",
        "    print(f\"Test: {len(test_idxes)} molecules\")\n",
        "\n",
        "    print(f\"TRAINING {model_type.upper()} MODEL\")\n",
        "    \n",
        "    start = time.time()\n",
        "    \n",
        "    # Create datasets and loaders based on model type\n",
        "    if model_type == 'gnn':\n",
        "        train_dataset = GeometricDataset(pos_data, type_data, fe, train_idx_split, config['threshold'])\n",
        "        val_dataset = GeometricDataset(pos_data, type_data, fe, val_idx_split, config['threshold'])\n",
        "        test_dataset = GeometricDataset(pos_data, type_data, fe, test_idxes, config['threshold'])\n",
        "        \n",
        "        train_loader = GeometricDataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
        "        val_loader = GeometricDataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "        test_loader = GeometricDataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
        "        \n",
        "        # Initialize GNN model\n",
        "        model = MolecularGNN(\n",
        "            num_atom_types=20, \n",
        "            hidden_dim=config['hidden_dim'], \n",
        "            num_layers=config['num_layers']\n",
        "        ).to(device)\n",
        "        \n",
        "    else:  # SMILES\n",
        "        train_dataset = SMILESDataset(smiles_data, fe, train_idx_split, char2idx, max_len)\n",
        "        val_dataset = SMILESDataset(smiles_data, fe, val_idx_split, char2idx, max_len)\n",
        "        test_dataset = SMILESDataset(smiles_data, fe, test_idxes, char2idx, max_len)\n",
        "        \n",
        "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
        "                                shuffle=True, collate_fn=smiles_collate_fn)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n",
        "                              shuffle=False, collate_fn=smiles_collate_fn)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], \n",
        "                                shuffle=False, collate_fn=smiles_collate_fn)\n",
        "        \n",
        "        # Initialize LSTM model\n",
        "        model = ImprovedLSTM(\n",
        "            vocab_size=vocab_size,\n",
        "            emb_dim=config['emb_dim'],\n",
        "            hidden_dim=config['hidden_dim'], \n",
        "            num_layers=config['num_layers'],\n",
        "            dropout=config['dropout']\n",
        "        ).to(device)\n",
        "    \n",
        "    print(f\"Data loading time: {time.time() - start:.2f} seconds\")\n",
        "    print(f\"{model_type.upper()} Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    \n",
        "    # Setup optimizer and scheduler\n",
        "    weight_decay = config.get('weight_decay', 0)  # Default 0 for GNN, specified for SMILES\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=config['scheduler_patience'], factor=config['scheduler_factor'])\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_metrics = []\n",
        "    \n",
        "    start = time.time()\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device, model_type)\n",
        "        \n",
        "        # Validate\n",
        "        val_results = evaluate(model, val_loader, criterion, device, model_type)\n",
        "        \n",
        "        # Store metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_metrics.append(val_results)\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_results['loss'])\n",
        "        \n",
        "        # Save best model\n",
        "        if val_results['loss'] < best_val_loss:\n",
        "            best_val_loss = val_results['loss']\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            if not use_efficiency_metrics:  # Only save for task 1\n",
        "                model_filename = generate_model_filename(config, model_type)\n",
        "                torch.save(model.state_dict(), model_filename)\n",
        "                print(f\"Saved new best {model_type.upper()} model: {model_filename}\")\n",
        "        \n",
        "        # Print progress\n",
        "        if use_efficiency_metrics:\n",
        "            metric1_info = val_results['metric1']\n",
        "            metric2_info = val_results['metric2']\n",
        "            print(f\"Epoch {epoch+1}: Loss={train_loss:.4f}, {metric1_info['name']}={metric1_info['value']:.3f}, {metric2_info['name']}={metric2_info['value']:.3f}\")\n",
        "        else:        \n",
        "            print(f\"Epoch {epoch+1}/{config['num_epochs']}:\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "            print(f\"  Val Loss: {val_results['loss']:.4f}\")\n",
        "            print(f\"  Val MAE (norm): {val_results['mae_norm']:.4f}, (unnorm): {val_results['mae_unnorm']:.4f}\")\n",
        "            print(f\"  Val MSE (norm): {val_results['mse_norm']:.4f}, (unnorm): {val_results['mse_unnorm']:.4f}\")\n",
        "            print(f\"  Val R²: {val_results['r2']:.4f}\")\n",
        "            print(f\"  Time elapsed: {time.time() - start:.2f} seconds\")\n",
        "\n",
        "    # Load best model and evaluate on test set\n",
        "    model_filename = generate_model_filename(config, model_type)\n",
        "    model.load_state_dict(torch.load(model_filename))\n",
        "    test_results = evaluate(model, test_loader, criterion, device, model_type)\n",
        "    \n",
        "\n",
        "    if use_efficiency_metrics:\n",
        "        metric1_info = test_results['metric1']\n",
        "        metric2_info = test_results['metric2']\n",
        "        print(f\"Final Test: {metric1_info['name']}={metric1_info['value']:.3f} {metric1_info['unit']}, {metric2_info['name']}={metric2_info['value']:.3f} {metric2_info['unit']}\")\n",
        "    else:\n",
        "        print(f\"FINAL {model_type.upper()} TEST RESULTS\")\n",
        "        print(f\"NORMALIZED METRICS:\")\n",
        "        print(f\"  Test MAE: {test_results['mae_norm']:.4f}\")\n",
        "        print(f\"  Test MSE: {test_results['mse_norm']:.4f}\")\n",
        "        print(f\"  Test R²:  {test_results['r2']:.4f}\")\n",
        "        print(f\"\\nUNNORMALIZED METRICS:\")\n",
        "        print(f\"  Test MAE: {test_results['mae_unnorm']:.4f}\")\n",
        "        print(f\"  Test MSE: {test_results['mse_unnorm']:.4f}\")\n",
        "        model_filename = generate_model_filename(config, model_type)\n",
        "        print(f\"\\nModel saved as: {model_filename}\")\n",
        "  \n",
        "    \n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_metrics': val_metrics,\n",
        "        'test_results': test_results,\n",
        "        'model_filename': generate_model_filename(config, model_type) if not use_efficiency_metrics else None,\n",
        "        'epochs_trained': len(train_losses),\n",
        "        'subset_size': len(training_indices) if subset_indices is not None else len(train_idxes)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG_GNN = {\n",
        "        # MODEL ARCHITECTURE\n",
        "        'hidden_dim': 64,\n",
        "        'num_layers': 3,\n",
        "        'threshold': 2.5,         # Distance threshold in Angstroms\n",
        "        \n",
        "        # TRAINING PARAMETERS\n",
        "        'lr': 1e-3,               \n",
        "        'batch_size': 32,         \n",
        "        'num_epochs': 2,         \n",
        "        'scheduler_patience': 10,\n",
        "        'scheduler_factor': 0.5,  \n",
        "        \n",
        "        # OTHER\n",
        "        'seed': 69,               \n",
        "        'val_split': 0.1,        #10%\n",
        "    }\n",
        "    \n",
        "    \n",
        "CONFIG_SMILES = {\n",
        "    # MODEL ARCHITECTURE\n",
        "    'emb_dim': 64,            \n",
        "    'hidden_dim': 128,        \n",
        "    'num_layers': 2,          \n",
        "    'dropout': 0.1,           \n",
        "    \n",
        "    # TRAINING PARAMETERS\n",
        "    'lr': 1e-3,               \n",
        "    'weight_decay': 1e-5,     # L2 regularization\n",
        "    'batch_size': 64,         \n",
        "    'num_epochs': 2,         \n",
        "    'scheduler_patience': 5,  \n",
        "    'scheduler_factor': 0.5,  \n",
        "    \n",
        "    # OTHER\n",
        "    'seed': 69,               \n",
        "    'val_split': 0.1,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#REMOVED FOR SUBMISSION (AS WE DONT RUN MODELS IN Q1)\n",
        "\n",
        "# Run training with the configuration\n",
        "results_GNN = run_training(CONFIG_GNN, 'gnn')\n",
        "print(f\"Model saved as: {results_GNN['model_filename']}\")\n",
        "\n",
        "# Plot results\n",
        "plot_results(results_GNN, CONFIG_GNN, 'gnn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#REMOVED FOR SUBMISSION (AS WE DONT RUN MODELS IN Q1)\n",
        "\n",
        "# Run training with the configuration\n",
        "results_SMILES = run_training(CONFIG_SMILES, 'smiles')\n",
        "print(f\"Model saved as: {results_SMILES['model_filename']}\")\n",
        "\n",
        "# Plot results\n",
        "plot_results(results_SMILES, CONFIG_SMILES, 'smiles')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zvKnpLGf9v"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The main train fuction for this question can be found at the end of Q1 as both of them do similar things, but 2 is more focused on efficeincy analysis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC1KICrZGgkY"
      },
      "outputs": [],
      "source": [
        "# # Define Metrics\n",
        "# def compute_mae_metric(targets, predictions, std_val, mu_val):\n",
        "#     \"\"\"Mean Absolute Error (unnormalized)\"\"\"\n",
        "#     mae_norm = mean_absolute_error(targets, predictions)\n",
        "#     mae_unnorm = mae_norm * std_val\n",
        "#     return mae_unnorm, \"MAE (eV)\"\n",
        "\n",
        "# def compute_pearson_correlation_metric(targets, predictions, std_val, mu_val):\n",
        "#     \"\"\"Pearson correlation coefficient\"\"\"\n",
        "#     correlation = np.corrcoef(targets, predictions)[0, 1]\n",
        "#     return correlation, \"Pearson r\"\n",
        "\n",
        "def compute_kendall_tau_metric(targets, predictions, std_val, mu_val):\n",
        "    \"\"\"Kendall's tau - ranking correlation for smaller samples\"\"\"\n",
        "    from scipy.stats import kendalltau\n",
        "    tau, _ = kendalltau(targets, predictions)\n",
        "    return tau, \"Kendall τ\"\n",
        "\n",
        "def compute_relative_rmse_metric(targets, predictions, std_val, mu_val):\n",
        "    \"\"\"RMSE as percentage of target range\"\"\"\n",
        "    rmse_norm = np.sqrt(mean_squared_error(targets, predictions))\n",
        "    rmse_unnorm = rmse_norm * std_val\n",
        "    target_range = (targets * std_val + mu_val).max() - (targets * std_val + mu_val).min()\n",
        "    relative_rmse = (rmse_unnorm / target_range) * 100\n",
        "    return relative_rmse, \"Rel. RMSE %\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get data subsets\n",
        "def create_data_subsets(train_idxes, subset_sizes, seed=69):\n",
        "    \"\"\"Create random subsets of training data for efficiency analysis\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    subsets = {}\n",
        "    \n",
        "    for size in subset_sizes:\n",
        "        if size > len(train_idxes):\n",
        "            print(f\"Warning: Requested size {size} > available data {len(train_idxes)}\")\n",
        "            subsets[size] = train_idxes.copy()\n",
        "        else:\n",
        "            # Randomly sample without replacement\n",
        "            subset_idx = np.random.choice(train_idxes, size=size, replace=False)\n",
        "            subsets[size] = subset_idx\n",
        "            \n",
        "    return subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_data_efficiency_results(gnn_results, smiles_results, subset_sizes):\n",
        "    \"\"\"\n",
        "    Plot data efficiency comparison between GNN and SMILES models using configured metrics\n",
        "    \"\"\"\n",
        "    # Get metric info from first successful result\n",
        "    metric1_info = None\n",
        "    metric2_info = None\n",
        "    \n",
        "    for size in subset_sizes:\n",
        "        if gnn_results[size] is not None:\n",
        "            metric1_info = gnn_results[size]['test_results']['metric1']\n",
        "            metric2_info = gnn_results[size]['test_results']['metric2']\n",
        "            break\n",
        "        elif smiles_results[size] is not None:\n",
        "            metric1_info = smiles_results[size]['test_results']['metric1']\n",
        "            metric2_info = smiles_results[size]['test_results']['metric2']\n",
        "            break\n",
        "    \n",
        "    if metric1_info is None or metric2_info is None:\n",
        "        print(\"Error: No successful results found for plotting\")\n",
        "        return\n",
        "    \n",
        "    # Extract metrics for plotting\n",
        "    gnn_metric1 = [gnn_results[size]['test_results']['metric1']['value'] if gnn_results[size] else np.nan \n",
        "                   for size in subset_sizes]\n",
        "    gnn_metric2 = [gnn_results[size]['test_results']['metric2']['value'] if gnn_results[size] else np.nan \n",
        "                   for size in subset_sizes]\n",
        "    \n",
        "    smiles_metric1 = [smiles_results[size]['test_results']['metric1']['value'] if smiles_results[size] else np.nan \n",
        "                      for size in subset_sizes]\n",
        "    smiles_metric2 = [smiles_results[size]['test_results']['metric2']['value'] if smiles_results[size] else np.nan \n",
        "                      for size in subset_sizes]\n",
        "    \n",
        "    # Create subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Plot Metric 1\n",
        "    ax1.plot(subset_sizes, gnn_metric1, 'o-', label='GNN', linewidth=2, markersize=8)\n",
        "    ax1.plot(subset_sizes, smiles_metric1, 's-', label='SMILES', linewidth=2, markersize=8)\n",
        "    ax1.set_xlabel('Training Set Size')\n",
        "    ax1.set_ylabel(f\"{metric1_info['name']} ({metric1_info['unit']})\")\n",
        "    ax1.set_title(f'Data Efficiency: {metric1_info[\"name\"]}')\n",
        "    ax1.set_xscale('log')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Plot Metric 2\n",
        "    ax2.plot(subset_sizes, gnn_metric2, 'o-', label='GNN', linewidth=2, markersize=8)\n",
        "    ax2.plot(subset_sizes, smiles_metric2, 's-', label='SMILES', linewidth=2, markersize=8)\n",
        "    ax2.set_xlabel('Training Set Size')\n",
        "    ax2.set_ylabel(f\"{metric2_info['name']} ({metric2_info['unit']})\")\n",
        "    ax2.set_title(f'Data Efficiency: {metric2_info[\"name\"]}')\n",
        "    ax2.set_xscale('log')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print summary table\n",
        "    print(\"\\n=== DATA EFFICIENCY SUMMARY ===\")\n",
        "    metric1_col = f\"GNN {metric1_info['name']}\"\n",
        "    metric2_col = f\"GNN {metric2_info['name']}\"\n",
        "    metric3_col = f\"SMILES {metric1_info['name']}\"\n",
        "    metric4_col = f\"SMILES {metric2_info['name']}\"\n",
        "    \n",
        "    print(f\"{'Size':<6} {metric1_col:<12} {metric2_col:<12} {metric3_col:<15} {metric4_col:<15}\")\n",
        "    print(\"-\" * 70)\n",
        "    \n",
        "    for i, size in enumerate(subset_sizes):\n",
        "        gnn_m1_val = f\"{gnn_metric1[i]:.3f}\" if not np.isnan(gnn_metric1[i]) else \"N/A\"\n",
        "        gnn_m2_val = f\"{gnn_metric2[i]:.3f}\" if not np.isnan(gnn_metric2[i]) else \"N/A\"\n",
        "        smiles_m1_val = f\"{smiles_metric1[i]:.3f}\" if not np.isnan(smiles_metric1[i]) else \"N/A\"\n",
        "        smiles_m2_val = f\"{smiles_metric2[i]:.3f}\" if not np.isnan(smiles_metric2[i]) else \"N/A\"\n",
        "        \n",
        "        print(f\"{size:<6} {gnn_m1_val:<12} {gnn_m2_val:<12} {smiles_m1_val:<15} {smiles_m2_val:<15}\")\n",
        "    \n",
        "    # Print metric units for clarity\n",
        "    print(f\"\\nMetrics: {metric1_info['name']} ({metric1_info['unit']}), {metric2_info['name']} ({metric2_info['unit']})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_data_efficiency_experiment(model_type='gnn', subset_sizes=[100, 300, 1000, 3000, 10000], config=CONFIG_GNN):\n",
        "    \"\"\"\n",
        "    Run data efficiency experiment for given model type across different training set sizes\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== DATA EFFICIENCY EXPERIMENT: {model_type.upper()} ===\")\n",
        "    \n",
        "    # Create data subsets\n",
        "    data_subsets = create_data_subsets(train_idxes, subset_sizes, seed=69)\n",
        "    \n",
        "    # Store results for each subset size\n",
        "    results = {}\n",
        "\n",
        "    \n",
        "    # Run experiment for each subset size\n",
        "    for size in subset_sizes:\n",
        "        print(f\"\\n--- Training {model_type.upper()} with {size} molecules ---\")\n",
        "        \n",
        "        # Get subset indices\n",
        "        subset_train_idx = data_subsets[size]\n",
        "        \n",
        "        try:\n",
        "            # Run training with subset using the adapted function\n",
        "            result = run_training(\n",
        "                config=config, \n",
        "                model_type=model_type, \n",
        "                subset_indices=subset_train_idx,\n",
        "                use_efficiency_metrics=True  # Use custom metrics for Task 2\n",
        "            )\n",
        "            results[size] = result\n",
        "            \n",
        "            # Print key metrics\n",
        "            test_results = result['test_results']\n",
        "            metric1_info = test_results['metric1']\n",
        "            metric2_info = test_results['metric2']\n",
        "            print(f\"Size {size}: {metric1_info['name']}={metric1_info['value']:.3f} {metric1_info['unit']}, \"\n",
        "                  f\"{metric2_info['name']}={metric2_info['value']:.3f} {metric2_info['unit']}\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Failed for size {size}: {e}\")\n",
        "            results[size] = None\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "EFFICIENCY_METRIC_CONFIG = {\n",
        "    'metric1': compute_relative_rmse_metric,\n",
        "    'metric2': compute_kendall_tau_metric,\n",
        "}\n",
        "\n",
        "SUBSET_SIZES = [100, 300, 1000, 3000, 10000]\n",
        "SUBSET_SIZES = [100, 300]\n",
        "# Configs from the two modeles in Q1 will be used to run the data efficiency analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run Q2 experiment\n",
        "# Run data efficiency experiments for both models\n",
        "gnn_efficiency_results = run_data_efficiency_experiment('gnn', SUBSET_SIZES, CONFIG_GNN)\n",
        "smiles_efficiency_results = run_data_efficiency_experiment('smiles', SUBSET_SIZES, CONFIG_SMILES)\n",
        "plot_data_efficiency_results(gnn_efficiency_results, smiles_efficiency_results, SUBSET_SIZES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mIuOY4BGxqU"
      },
      "source": [
        "## Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_NgxsO3GxEE"
      },
      "outputs": [],
      "source": [
        "def is_valid_smiles(smiles):\n",
        "    if smiles is None:\n",
        "        return False\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        return mol is not None\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def canonicalize(smiles):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol:\n",
        "            return Chem.MolToSmiles(mol, canonical=True)\n",
        "        return 'None'\n",
        "    except:\n",
        "        return 'None'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SN_jGgOwG4kK"
      },
      "outputs": [],
      "source": [
        "canonicalize(\"COO\"), canonicalize(\"O(C)O\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bhjYhYrHCuQ"
      },
      "outputs": [],
      "source": [
        "is_valid_smiles(\"COO\"), is_valid_smiles(\"O(C)O\"), is_valid_smiles(\"C##\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_YgzDpMH-Vl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from rdkit import Chem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Build gen-vocab (reuse your char2idx from before + EOS)\n",
        "gen_char2idx = dict(char2idx)               # from your code\n",
        "gen_char2idx['<SOS>'] = len(gen_char2idx)\n",
        "gen_char2idx['<EOS>'] = len(gen_char2idx)\n",
        "PAD = gen_char2idx['<PAD>']\n",
        "SOS = gen_char2idx['<SOS>']\n",
        "EOS = gen_char2idx['<EOS>']\n",
        "gen_idx2char = {i:c for c,i in gen_char2idx.items()}\n",
        "V = len(gen_char2idx)\n",
        "\n",
        "# 2) Prepare next-char training sequences\n",
        "train_smiles = [smiles_data[i] for i in train_idxes]\n",
        "sequences = []\n",
        "for smi in train_smiles:\n",
        "    # add SOS/EOS\n",
        "    seq = [SOS] + [gen_char2idx.get(c, char2idx['<UNK>']) for c in smi] + [EOS]\n",
        "    sequences.append(seq)\n",
        "\n",
        "max_gen_len = max(len(s) for s in sequences)\n",
        "\n",
        "# pad and build inputs/targets\n",
        "inputs, targets = [], []\n",
        "for seq in sequences:\n",
        "    padded = seq + [PAD]*(max_gen_len - len(seq))\n",
        "    inp = padded[:-1]\n",
        "    tgt = padded[1:]\n",
        "    inputs.append(inp)\n",
        "    targets.append(tgt)\n",
        "\n",
        "inputs  = torch.tensor(inputs,  dtype=torch.long)\n",
        "targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "# 3) Dataset & DataLoader\n",
        "class LMDS(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "    def __len__(self): return len(self.X)\n",
        "    def __getitem__(self, i): return self.X[i], self.Y[i]\n",
        "\n",
        "batch_size = 256\n",
        "lm_loader = DataLoader(LMDS(inputs, targets),\n",
        "                       batch_size=batch_size,\n",
        "                       shuffle=True)\n",
        "\n",
        "# 4) Char-LSTM language model\n",
        "class CharLSTM(nn.Module):\n",
        "    def __init__(self, V, emb_dim=64, hid_dim=256, nlayers=2):\n",
        "        super().__init__()\n",
        "        self.emb  = nn.Embedding(V, emb_dim, padding_idx=PAD)\n",
        "        self.lstm = nn.LSTM(emb_dim, hid_dim,\n",
        "                            num_layers=nlayers,\n",
        "                            batch_first=True)\n",
        "        self.fc   = nn.Linear(hid_dim, V)\n",
        "    def forward(self, x):\n",
        "        e, _ = self.lstm(self.emb(x))\n",
        "        return self.fc(e)   # (B, T, V)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "lm_model = CharLSTM(V).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(lm_model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "\n",
        "# 5) Train the LM\n",
        "n_epochs = 10\n",
        "print(\"\\nTraining SMILES language model:\")\n",
        "for ep in range(1, n_epochs+1):\n",
        "    lm_model.train()\n",
        "    total_loss = 0\n",
        "    for Xb, Yb in tqdm(lm_loader, desc=f\"Epoch {ep}\", leave=False):\n",
        "        Xb, Yb = Xb.to(device), Yb.to(device)\n",
        "        logits = lm_model(Xb)              # B×T×V\n",
        "        loss = criterion(logits.view(-1, V), Yb.view(-1))\n",
        "        optimizer.zero_grad(); loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * Xb.size(0)\n",
        "    print(f\" Ep {ep}/{n_epochs}  loss = {total_loss/len(lm_loader.dataset):.4f}\")\n",
        "\n",
        "# 6) Sampling function\n",
        "def sample_smiles(model, max_len=200, temp=1.0):\n",
        "    model.eval()\n",
        "    seq = [SOS]\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            x = torch.tensor([seq], device=device)\n",
        "            logits = model(x)[0, -1] / temp\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx = torch.multinomial(probs, 1).item()\n",
        "            if idx == EOS:\n",
        "                break\n",
        "            seq.append(idx)\n",
        "    # decode (skip SOS)\n",
        "    return ''.join(gen_idx2char[i] for i in seq[1:])\n",
        "\n",
        "# 7) Generate & compute metrics\n",
        "N = 5000\n",
        "print(f\"\\nSampling exactly {N} SMILES…\")\n",
        "gen = []\n",
        "for _ in tqdm(range(N), desc=\"Sampling 5K\"):\n",
        "    smi = sample_smiles(lm_model, max_len=max_gen_len, temp=1.0)\n",
        "    gen.append(smi)\n",
        "\n",
        "# filter valid\n",
        "valid = [s for s in gen if Chem.MolFromSmiles(s) is not None]\n",
        "val_count = len(valid)\n",
        "validity = val_count / len(gen)\n",
        "\n",
        "# uniqueness on valid\n",
        "unique_valid = list(dict.fromkeys(valid))\n",
        "uniqueness = len(unique_valid) / val_count\n",
        "\n",
        "# novelty: not in training set\n",
        "train_set = set(train_smiles)\n",
        "novel = [s for s in unique_valid if s not in train_set]\n",
        "novelty = len(novel) / len(unique_valid)\n",
        "\n",
        "print(\"\\nTask 3 SMILES‐generation metrics:\")\n",
        "print(f\"  Generated total  : {len(gen)}\")\n",
        "print(f\"  Validity         : {validity:.3f}   ({val_count}/{len(gen)})\")\n",
        "print(f\"  Uniqueness       : {uniqueness:.3f}   ({len(unique_valid)}/{val_count})\")\n",
        "print(f\"  Novelty          : {novelty:.3f}   ({len(novel)}/{len(unique_valid)})\")\n",
        "\n",
        "# if you want exactly 5000 molecules to hand in:\n",
        "final_smiles = unique_valid[:5000]\n",
        "# you can save or print them:\n",
        "with open('generated_5000.txt','w') as f:\n",
        "    for s in final_smiles:\n",
        "        f.write(s+\"\\n\")\n",
        "print(\"\\nSaved 5000 unique valid SMILES to generated_5000.txt\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
